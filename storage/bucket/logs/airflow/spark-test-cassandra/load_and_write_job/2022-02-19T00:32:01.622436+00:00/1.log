[2022-02-19 00:32:37,870] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: spark-test-cassandra.load_and_write_job manual__2022-02-19T00:32:01.622436+00:00 [queued]>
[2022-02-19 00:32:37,882] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: spark-test-cassandra.load_and_write_job manual__2022-02-19T00:32:01.622436+00:00 [queued]>
[2022-02-19 00:32:37,882] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2022-02-19 00:32:37,882] {taskinstance.py:1239} INFO - Starting attempt 1 of 1
[2022-02-19 00:32:37,882] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2022-02-19 00:32:37,894] {taskinstance.py:1259} INFO - Executing <Task(BashOperator): load_and_write_job> on 2022-02-19 00:32:01.622436+00:00
[2022-02-19 00:32:37,899] {standard_task_runner.py:52} INFO - Started process 278 to run task
[2022-02-19 00:32:37,902] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'spark-test-cassandra', 'load_and_write_job', 'manual__2022-02-19T00:32:01.622436+00:00', '--job-id', '8', '--raw', '--subdir', 'DAGS_FOLDER/spark-cassandra-dag.py', '--cfg-path', '/tmp/tmp5x_dg65b', '--error-file', '/tmp/tmp1axq581l']
[2022-02-19 00:32:37,902] {standard_task_runner.py:77} INFO - Job 8: Subtask load_and_write_job
[2022-02-19 00:32:37,949] {logging_mixin.py:109} INFO - Running <TaskInstance: spark-test-cassandra.load_and_write_job manual__2022-02-19T00:32:01.622436+00:00 [running]> on host 2e85b2b3b082
[2022-02-19 00:32:38,000] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Airflow
AIRFLOW_CTX_DAG_ID=spark-test-cassandra
AIRFLOW_CTX_TASK_ID=load_and_write_job
AIRFLOW_CTX_EXECUTION_DATE=2022-02-19T00:32:01.622436+00:00
AIRFLOW_CTX_DAG_RUN_ID=manual__2022-02-19T00:32:01.622436+00:00
[2022-02-19 00:32:38,001] {subprocess.py:62} INFO - Tmp dir root location: 
 /tmp
[2022-02-19 00:32:38,002] {subprocess.py:74} INFO - Running command: ['bash', '-c', 'spark-submit             --packages com.datastax.spark:spark-cassandra-connector_2.12:3.1.0             /usr/local/spark/app/spark-cassandra-test.py']
[2022-02-19 00:32:38,013] {subprocess.py:85} INFO - Output:
[2022-02-19 00:32:39,948] {subprocess.py:89} INFO - WARNING: An illegal reflective access operation has occurred
[2022-02-19 00:32:39,948] {subprocess.py:89} INFO - WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/***/.local/lib/python3.7/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)
[2022-02-19 00:32:39,948] {subprocess.py:89} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[2022-02-19 00:32:39,949] {subprocess.py:89} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2022-02-19 00:32:39,949] {subprocess.py:89} INFO - WARNING: All illegal access operations will be denied in a future release
[2022-02-19 00:32:40,134] {subprocess.py:89} INFO - :: loading settings :: url = jar:file:/home/***/.local/lib/python3.7/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2022-02-19 00:32:40,257] {subprocess.py:89} INFO - Ivy Default Cache set to: /home/***/.ivy2/cache
[2022-02-19 00:32:40,257] {subprocess.py:89} INFO - The jars for the packages stored in: /home/***/.ivy2/jars
[2022-02-19 00:32:40,262] {subprocess.py:89} INFO - com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency
[2022-02-19 00:32:40,264] {subprocess.py:89} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-987615d0-fe8d-4d75-a366-e7f601137028;1.0
[2022-02-19 00:32:40,264] {subprocess.py:89} INFO - 	confs: [default]
[2022-02-19 00:32:42,636] {subprocess.py:89} INFO - 	found com.datastax.spark#spark-cassandra-connector_2.12;3.1.0 in central
[2022-02-19 00:32:43,607] {subprocess.py:89} INFO - 	found com.datastax.spark#spark-cassandra-connector-driver_2.12;3.1.0 in central
[2022-02-19 00:32:49,877] {subprocess.py:89} INFO - 	found com.datastax.oss#java-driver-core-shaded;4.12.0 in central
[2022-02-19 00:32:50,897] {subprocess.py:89} INFO - 	found com.datastax.oss#native-protocol;1.5.0 in central
[2022-02-19 00:32:51,816] {subprocess.py:89} INFO - 	found com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central
[2022-02-19 00:32:52,841] {subprocess.py:89} INFO - 	found com.typesafe#config;1.4.1 in central
[2022-02-19 00:32:55,195] {subprocess.py:89} INFO - 	found org.slf4j#slf4j-api;1.7.26 in central
[2022-02-19 00:32:57,651] {subprocess.py:89} INFO - 	found io.dropwizard.metrics#metrics-core;4.1.18 in central
[2022-02-19 00:32:58,610] {subprocess.py:89} INFO - 	found org.hdrhistogram#HdrHistogram;2.1.12 in central
[2022-02-19 00:32:59,598] {subprocess.py:89} INFO - 	found org.reactivestreams#reactive-streams;1.0.3 in central
[2022-02-19 00:33:01,998] {subprocess.py:89} INFO - 	found com.github.stephenc.jcip#jcip-annotations;1.0-1 in central
[2022-02-19 00:33:02,978] {subprocess.py:89} INFO - 	found com.github.spotbugs#spotbugs-annotations;3.1.12 in central
[2022-02-19 00:33:03,999] {subprocess.py:89} INFO - 	found com.google.code.findbugs#jsr305;3.0.2 in central
[2022-02-19 00:33:05,130] {subprocess.py:89} INFO - 	found com.datastax.oss#java-driver-mapper-runtime;4.12.0 in central
[2022-02-19 00:33:06,151] {subprocess.py:89} INFO - 	found com.datastax.oss#java-driver-query-builder;4.12.0 in central
[2022-02-19 00:33:10,042] {subprocess.py:89} INFO - 	found org.apache.commons#commons-lang3;3.10 in central
[2022-02-19 00:33:14,333] {subprocess.py:89} INFO - 	found com.thoughtworks.paranamer#paranamer;2.8 in central
[2022-02-19 00:33:15,570] {subprocess.py:89} INFO - 	found org.scala-lang#scala-reflect;2.12.11 in central
[2022-02-19 00:33:15,594] {subprocess.py:89} INFO - downloading https://repo1.maven.org/maven2/com/datastax/spark/spark-cassandra-connector_2.12/3.1.0/spark-cassandra-connector_2.12-3.1.0.jar ...
[2022-02-19 00:33:16,922] {subprocess.py:89} INFO - 	[SUCCESSFUL ] com.datastax.spark#spark-cassandra-connector_2.12;3.1.0!spark-cassandra-connector_2.12.jar (1327ms)
[2022-02-19 00:33:16,922] {subprocess.py:89} INFO - downloading https://repo1.maven.org/maven2/com/datastax/spark/spark-cassandra-connector-driver_2.12/3.1.0/spark-cassandra-connector-driver_2.12-3.1.0.jar ...
[2022-02-19 00:33:17,669] {subprocess.py:89} INFO - 	[SUCCESSFUL ] com.datastax.spark#spark-cassandra-connector-driver_2.12;3.1.0!spark-cassandra-connector-driver_2.12.jar (747ms)
[2022-02-19 00:33:17,670] {subprocess.py:89} INFO - downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-core-shaded/4.12.0/java-driver-core-shaded-4.12.0.jar ...
[2022-02-19 00:33:20,341] {subprocess.py:89} INFO - 	[SUCCESSFUL ] com.datastax.oss#java-driver-core-shaded;4.12.0!java-driver-core-shaded.jar (2671ms)
[2022-02-19 00:33:20,342] {subprocess.py:89} INFO - downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-mapper-runtime/4.12.0/java-driver-mapper-runtime-4.12.0.jar ...
[2022-02-19 00:33:20,588] {subprocess.py:89} INFO - 	[SUCCESSFUL ] com.datastax.oss#java-driver-mapper-runtime;4.12.0!java-driver-mapper-runtime.jar(bundle) (245ms)
[2022-02-19 00:33:20,588] {subprocess.py:89} INFO - downloading https://repo1.maven.org/maven2/org/apache/commons/commons-lang3/3.10/commons-lang3-3.10.jar ...
[2022-02-19 00:33:20,847] {subprocess.py:89} INFO - 	[SUCCESSFUL ] org.apache.commons#commons-lang3;3.10!commons-lang3.jar (258ms)
[2022-02-19 00:33:20,848] {subprocess.py:89} INFO - downloading https://repo1.maven.org/maven2/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar ...
[2022-02-19 00:33:21,102] {subprocess.py:89} INFO - 	[SUCCESSFUL ] com.thoughtworks.paranamer#paranamer;2.8!paranamer.jar(bundle) (254ms)
[2022-02-19 00:33:21,103] {subprocess.py:89} INFO - downloading https://repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.11/scala-reflect-2.12.11.jar ...
[2022-02-19 00:33:22,018] {subprocess.py:89} INFO - 	[SUCCESSFUL ] org.scala-lang#scala-reflect;2.12.11!scala-reflect.jar (914ms)
[2022-02-19 00:33:22,018] {subprocess.py:89} INFO - downloading https://repo1.maven.org/maven2/com/datastax/oss/native-protocol/1.5.0/native-protocol-1.5.0.jar ...
[2022-02-19 00:33:22,233] {subprocess.py:89} INFO - 	[SUCCESSFUL ] com.datastax.oss#native-protocol;1.5.0!native-protocol.jar(bundle) (215ms)
[2022-02-19 00:33:22,234] {subprocess.py:89} INFO - downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-shaded-guava/25.1-jre-graal-sub-1/java-driver-shaded-guava-25.1-jre-graal-sub-1.jar ...
[2022-02-19 00:33:22,889] {subprocess.py:89} INFO - 	[SUCCESSFUL ] com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1!java-driver-shaded-guava.jar (655ms)
[2022-02-19 00:33:22,890] {subprocess.py:89} INFO - downloading https://repo1.maven.org/maven2/com/typesafe/config/1.4.1/config-1.4.1.jar ...
[2022-02-19 00:33:23,097] {subprocess.py:89} INFO - 	[SUCCESSFUL ] com.typesafe#config;1.4.1!config.jar(bundle) (207ms)
[2022-02-19 00:33:23,098] {subprocess.py:89} INFO - downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.26/slf4j-api-1.7.26.jar ...
[2022-02-19 00:33:23,352] {subprocess.py:89} INFO - 	[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.26!slf4j-api.jar (254ms)
[2022-02-19 00:33:23,353] {subprocess.py:89} INFO - downloading https://repo1.maven.org/maven2/io/dropwizard/metrics/metrics-core/4.1.18/metrics-core-4.1.18.jar ...
[2022-02-19 00:33:23,558] {subprocess.py:89} INFO - 	[SUCCESSFUL ] io.dropwizard.metrics#metrics-core;4.1.18!metrics-core.jar(bundle) (205ms)
[2022-02-19 00:33:23,558] {subprocess.py:89} INFO - downloading https://repo1.maven.org/maven2/org/hdrhistogram/HdrHistogram/2.1.12/HdrHistogram-2.1.12.jar ...
[2022-02-19 00:33:23,769] {subprocess.py:89} INFO - 	[SUCCESSFUL ] org.hdrhistogram#HdrHistogram;2.1.12!HdrHistogram.jar(bundle) (211ms)
[2022-02-19 00:33:23,770] {subprocess.py:89} INFO - downloading https://repo1.maven.org/maven2/org/reactivestreams/reactive-streams/1.0.3/reactive-streams-1.0.3.jar ...
[2022-02-19 00:33:23,964] {subprocess.py:89} INFO - 	[SUCCESSFUL ] org.reactivestreams#reactive-streams;1.0.3!reactive-streams.jar (195ms)
[2022-02-19 00:33:23,965] {subprocess.py:89} INFO - downloading https://repo1.maven.org/maven2/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar ...
[2022-02-19 00:33:24,169] {subprocess.py:89} INFO - 	[SUCCESSFUL ] com.github.stephenc.jcip#jcip-annotations;1.0-1!jcip-annotations.jar (204ms)
[2022-02-19 00:33:24,170] {subprocess.py:89} INFO - downloading https://repo1.maven.org/maven2/com/github/spotbugs/spotbugs-annotations/3.1.12/spotbugs-annotations-3.1.12.jar ...
[2022-02-19 00:33:24,316] {subprocess.py:89} INFO - 	[SUCCESSFUL ] com.github.spotbugs#spotbugs-annotations;3.1.12!spotbugs-annotations.jar (146ms)
[2022-02-19 00:33:24,317] {subprocess.py:89} INFO - downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.2/jsr305-3.0.2.jar ...
[2022-02-19 00:33:24,460] {subprocess.py:89} INFO - 	[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.2!jsr305.jar (143ms)
[2022-02-19 00:33:24,461] {subprocess.py:89} INFO - downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-query-builder/4.12.0/java-driver-query-builder-4.12.0.jar ...
[2022-02-19 00:33:24,687] {subprocess.py:89} INFO - 	[SUCCESSFUL ] com.datastax.oss#java-driver-query-builder;4.12.0!java-driver-query-builder.jar(bundle) (226ms)
[2022-02-19 00:33:24,688] {subprocess.py:89} INFO - :: resolution report :: resolve 35329ms :: artifacts dl 9095ms
[2022-02-19 00:33:24,688] {subprocess.py:89} INFO - 	:: modules in use:
[2022-02-19 00:33:24,688] {subprocess.py:89} INFO - 	com.datastax.oss#java-driver-core-shaded;4.12.0 from central in [default]
[2022-02-19 00:33:24,688] {subprocess.py:89} INFO - 	com.datastax.oss#java-driver-mapper-runtime;4.12.0 from central in [default]
[2022-02-19 00:33:24,688] {subprocess.py:89} INFO - 	com.datastax.oss#java-driver-query-builder;4.12.0 from central in [default]
[2022-02-19 00:33:24,688] {subprocess.py:89} INFO - 	com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]
[2022-02-19 00:33:24,689] {subprocess.py:89} INFO - 	com.datastax.oss#native-protocol;1.5.0 from central in [default]
[2022-02-19 00:33:24,689] {subprocess.py:89} INFO - 	com.datastax.spark#spark-cassandra-connector-driver_2.12;3.1.0 from central in [default]
[2022-02-19 00:33:24,689] {subprocess.py:89} INFO - 	com.datastax.spark#spark-cassandra-connector_2.12;3.1.0 from central in [default]
[2022-02-19 00:33:24,689] {subprocess.py:89} INFO - 	com.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]
[2022-02-19 00:33:24,689] {subprocess.py:89} INFO - 	com.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]
[2022-02-19 00:33:24,689] {subprocess.py:89} INFO - 	com.google.code.findbugs#jsr305;3.0.2 from central in [default]
[2022-02-19 00:33:24,689] {subprocess.py:89} INFO - 	com.thoughtworks.paranamer#paranamer;2.8 from central in [default]
[2022-02-19 00:33:24,689] {subprocess.py:89} INFO - 	com.typesafe#config;1.4.1 from central in [default]
[2022-02-19 00:33:24,689] {subprocess.py:89} INFO - 	io.dropwizard.metrics#metrics-core;4.1.18 from central in [default]
[2022-02-19 00:33:24,689] {subprocess.py:89} INFO - 	org.apache.commons#commons-lang3;3.10 from central in [default]
[2022-02-19 00:33:24,690] {subprocess.py:89} INFO - 	org.hdrhistogram#HdrHistogram;2.1.12 from central in [default]
[2022-02-19 00:33:24,690] {subprocess.py:89} INFO - 	org.reactivestreams#reactive-streams;1.0.3 from central in [default]
[2022-02-19 00:33:24,690] {subprocess.py:89} INFO - 	org.scala-lang#scala-reflect;2.12.11 from central in [default]
[2022-02-19 00:33:24,690] {subprocess.py:89} INFO - 	org.slf4j#slf4j-api;1.7.26 from central in [default]
[2022-02-19 00:33:24,690] {subprocess.py:89} INFO - 	---------------------------------------------------------------------
[2022-02-19 00:33:24,690] {subprocess.py:89} INFO - 	|                  |            modules            ||   artifacts   |
[2022-02-19 00:33:24,690] {subprocess.py:89} INFO - 	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2022-02-19 00:33:24,690] {subprocess.py:89} INFO - 	---------------------------------------------------------------------
[2022-02-19 00:33:24,690] {subprocess.py:89} INFO - 	|      default     |   18  |   18  |   18  |   0   ||   18  |   18  |
[2022-02-19 00:33:24,690] {subprocess.py:89} INFO - 	---------------------------------------------------------------------
[2022-02-19 00:33:24,697] {subprocess.py:89} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-987615d0-fe8d-4d75-a366-e7f601137028
[2022-02-19 00:33:24,697] {subprocess.py:89} INFO - 	confs: [default]
[2022-02-19 00:33:24,734] {subprocess.py:89} INFO - 	18 artifacts copied, 0 already retrieved (18047kB/37ms)
[2022-02-19 00:33:25,126] {subprocess.py:89} INFO - 22/02/19 00:33:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2022-02-19 00:33:26,612] {subprocess.py:89} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2022-02-19 00:33:26,621] {subprocess.py:89} INFO - 22/02/19 00:33:26 INFO SparkContext: Running Spark version 3.2.0
[2022-02-19 00:33:26,650] {subprocess.py:89} INFO - 22/02/19 00:33:26 INFO ResourceUtils: ==============================================================
[2022-02-19 00:33:26,651] {subprocess.py:89} INFO - 22/02/19 00:33:26 INFO ResourceUtils: No custom resources configured for spark.driver.
[2022-02-19 00:33:26,651] {subprocess.py:89} INFO - 22/02/19 00:33:26 INFO ResourceUtils: ==============================================================
[2022-02-19 00:33:26,651] {subprocess.py:89} INFO - 22/02/19 00:33:26 INFO SparkContext: Submitted application: extract_and_load
[2022-02-19 00:33:26,682] {subprocess.py:89} INFO - 22/02/19 00:33:26 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2022-02-19 00:33:26,699] {subprocess.py:89} INFO - 22/02/19 00:33:26 INFO ResourceProfile: Limiting resource is cpu
[2022-02-19 00:33:26,700] {subprocess.py:89} INFO - 22/02/19 00:33:26 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2022-02-19 00:33:26,816] {subprocess.py:89} INFO - 22/02/19 00:33:26 INFO SecurityManager: Changing view acls to: ***
[2022-02-19 00:33:26,821] {subprocess.py:89} INFO - 22/02/19 00:33:26 INFO SecurityManager: Changing modify acls to: ***
[2022-02-19 00:33:26,822] {subprocess.py:89} INFO - 22/02/19 00:33:26 INFO SecurityManager: Changing view acls groups to:
[2022-02-19 00:33:26,823] {subprocess.py:89} INFO - 22/02/19 00:33:26 INFO SecurityManager: Changing modify acls groups to:
[2022-02-19 00:33:26,825] {subprocess.py:89} INFO - 22/02/19 00:33:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(***); groups with view permissions: Set(); users  with modify permissions: Set(***); groups with modify permissions: Set()
[2022-02-19 00:33:27,733] {subprocess.py:89} INFO - 22/02/19 00:33:27 INFO Utils: Successfully started service 'sparkDriver' on port 35165.
[2022-02-19 00:33:27,873] {subprocess.py:89} INFO - 22/02/19 00:33:27 INFO SparkEnv: Registering MapOutputTracker
[2022-02-19 00:33:27,956] {subprocess.py:89} INFO - 22/02/19 00:33:27 INFO SparkEnv: Registering BlockManagerMaster
[2022-02-19 00:33:27,992] {subprocess.py:89} INFO - 22/02/19 00:33:27 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2022-02-19 00:33:27,993] {subprocess.py:89} INFO - 22/02/19 00:33:27 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2022-02-19 00:33:28,000] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2022-02-19 00:33:28,056] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-fa0280c7-8767-4151-84d8-d8ca4aa539ed
[2022-02-19 00:33:28,098] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2022-02-19 00:33:28,123] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO SparkEnv: Registering OutputCommitCoordinator
[2022-02-19 00:33:28,374] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2022-02-19 00:33:28,445] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://2e85b2b3b082:4040
[2022-02-19 00:33:28,463] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.1.0.jar at spark://2e85b2b3b082:35165/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.1.0.jar with timestamp 1645230806608
[2022-02-19 00:33:28,463] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.1.0.jar at spark://2e85b2b3b082:35165/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.1.0.jar with timestamp 1645230806608
[2022-02-19 00:33:28,463] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.12.0.jar at spark://2e85b2b3b082:35165/jars/com.datastax.oss_java-driver-core-shaded-4.12.0.jar with timestamp 1645230806608
[2022-02-19 00:33:28,464] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.12.0.jar at spark://2e85b2b3b082:35165/jars/com.datastax.oss_java-driver-mapper-runtime-4.12.0.jar with timestamp 1645230806608
[2022-02-19 00:33:28,464] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar at spark://2e85b2b3b082:35165/jars/org.apache.commons_commons-lang3-3.10.jar with timestamp 1645230806608
[2022-02-19 00:33:28,464] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar at spark://2e85b2b3b082:35165/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1645230806608
[2022-02-19 00:33:28,464] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar at spark://2e85b2b3b082:35165/jars/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1645230806608
[2022-02-19 00:33:28,464] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar at spark://2e85b2b3b082:35165/jars/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1645230806608
[2022-02-19 00:33:28,464] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar at spark://2e85b2b3b082:35165/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1645230806608
[2022-02-19 00:33:28,464] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.typesafe_config-1.4.1.jar at spark://2e85b2b3b082:35165/jars/com.typesafe_config-1.4.1.jar with timestamp 1645230806608
[2022-02-19 00:33:28,464] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar at spark://2e85b2b3b082:35165/jars/org.slf4j_slf4j-api-1.7.26.jar with timestamp 1645230806608
[2022-02-19 00:33:28,465] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar at spark://2e85b2b3b082:35165/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1645230806608
[2022-02-19 00:33:28,465] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar at spark://2e85b2b3b082:35165/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1645230806608
[2022-02-19 00:33:28,465] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar at spark://2e85b2b3b082:35165/jars/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1645230806608
[2022-02-19 00:33:28,465] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at spark://2e85b2b3b082:35165/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1645230806608
[2022-02-19 00:33:28,465] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar at spark://2e85b2b3b082:35165/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1645230806608
[2022-02-19 00:33:28,465] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar at spark://2e85b2b3b082:35165/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1645230806608
[2022-02-19 00:33:28,465] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.12.0.jar at spark://2e85b2b3b082:35165/jars/com.datastax.oss_java-driver-query-builder-4.12.0.jar with timestamp 1645230806608
[2022-02-19 00:33:28,469] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.1.0.jar at file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.1.0.jar with timestamp 1645230806608
[2022-02-19 00:33:28,470] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.1.0.jar to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.datastax.spark_spark-cassandra-connector_2.12-3.1.0.jar
[2022-02-19 00:33:28,483] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.1.0.jar at file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.1.0.jar with timestamp 1645230806608
[2022-02-19 00:33:28,483] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.1.0.jar to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.1.0.jar
[2022-02-19 00:33:28,489] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.12.0.jar at file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.12.0.jar with timestamp 1645230806608
[2022-02-19 00:33:28,489] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.12.0.jar to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.datastax.oss_java-driver-core-shaded-4.12.0.jar
[2022-02-19 00:33:28,506] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.12.0.jar at file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.12.0.jar with timestamp 1645230806608
[2022-02-19 00:33:28,506] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.12.0.jar to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.datastax.oss_java-driver-mapper-runtime-4.12.0.jar
[2022-02-19 00:33:28,512] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar at file:///home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar with timestamp 1645230806608
[2022-02-19 00:33:28,512] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/org.apache.commons_commons-lang3-3.10.jar
[2022-02-19 00:33:28,517] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar at file:///home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1645230806608
[2022-02-19 00:33:28,517] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Utils: Copying /home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.thoughtworks.paranamer_paranamer-2.8.jar
[2022-02-19 00:33:28,524] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar at file:///home/***/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1645230806608
[2022-02-19 00:33:28,524] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Utils: Copying /home/***/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/org.scala-lang_scala-reflect-2.12.11.jar
[2022-02-19 00:33:28,532] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar at file:///home/***/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1645230806608
[2022-02-19 00:33:28,532] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.datastax.oss_native-protocol-1.5.0.jar
[2022-02-19 00:33:28,536] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar at file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1645230806608
[2022-02-19 00:33:28,537] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar
[2022-02-19 00:33:28,544] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.typesafe_config-1.4.1.jar at file:///home/***/.ivy2/jars/com.typesafe_config-1.4.1.jar with timestamp 1645230806608
[2022-02-19 00:33:28,544] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Utils: Copying /home/***/.ivy2/jars/com.typesafe_config-1.4.1.jar to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.typesafe_config-1.4.1.jar
[2022-02-19 00:33:28,548] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar at file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar with timestamp 1645230806608
[2022-02-19 00:33:28,548] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Utils: Copying /home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/org.slf4j_slf4j-api-1.7.26.jar
[2022-02-19 00:33:28,552] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO SparkContext: Added file file:///home/***/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar at file:///home/***/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1645230806608
[2022-02-19 00:33:28,552] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Utils: Copying /home/***/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/io.dropwizard.metrics_metrics-core-4.1.18.jar
[2022-02-19 00:33:28,559] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar at file:///home/***/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1645230806608
[2022-02-19 00:33:28,559] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Utils: Copying /home/***/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/org.hdrhistogram_HdrHistogram-2.1.12.jar
[2022-02-19 00:33:28,566] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar at file:///home/***/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1645230806608
[2022-02-19 00:33:28,566] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Utils: Copying /home/***/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/org.reactivestreams_reactive-streams-1.0.3.jar
[2022-02-19 00:33:28,571] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar at file:///home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1645230806608
[2022-02-19 00:33:28,577] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Utils: Copying /home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar
[2022-02-19 00:33:28,577] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar at file:///home/***/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1645230806608
[2022-02-19 00:33:28,578] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Utils: Copying /home/***/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.github.spotbugs_spotbugs-annotations-3.1.12.jar
[2022-02-19 00:33:28,582] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar at file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1645230806608
[2022-02-19 00:33:28,582] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Utils: Copying /home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.google.code.findbugs_jsr305-3.0.2.jar
[2022-02-19 00:33:28,586] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.12.0.jar at file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.12.0.jar with timestamp 1645230806608
[2022-02-19 00:33:28,587] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Utils: Copying /home/***/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.12.0.jar to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.datastax.oss_java-driver-query-builder-4.12.0.jar
[2022-02-19 00:33:28,799] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Executor: Starting executor ID driver on host 2e85b2b3b082
[2022-02-19 00:33:28,819] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Executor: Fetching file:///home/***/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1645230806608
[2022-02-19 00:33:28,843] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Utils: /home/***/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar has been previously copied to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.datastax.oss_native-protocol-1.5.0.jar
[2022-02-19 00:33:28,847] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Executor: Fetching file:///home/***/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1645230806608
[2022-02-19 00:33:28,848] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Utils: /home/***/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar has been previously copied to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.github.spotbugs_spotbugs-annotations-3.1.12.jar
[2022-02-19 00:33:28,851] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Executor: Fetching file:///home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1645230806608
[2022-02-19 00:33:28,852] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Utils: /home/***/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar has been previously copied to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.google.code.findbugs_jsr305-3.0.2.jar
[2022-02-19 00:33:28,855] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Executor: Fetching file:///home/***/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1645230806608
[2022-02-19 00:33:28,856] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Utils: /home/***/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar has been previously copied to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/io.dropwizard.metrics_metrics-core-4.1.18.jar
[2022-02-19 00:33:28,859] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Executor: Fetching file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.1.0.jar with timestamp 1645230806608
[2022-02-19 00:33:28,861] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Utils: /home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.1.0.jar has been previously copied to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.1.0.jar
[2022-02-19 00:33:28,864] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Executor: Fetching file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar with timestamp 1645230806608
[2022-02-19 00:33:28,864] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Utils: /home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar has been previously copied to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/org.slf4j_slf4j-api-1.7.26.jar
[2022-02-19 00:33:28,868] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Executor: Fetching file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1645230806608
[2022-02-19 00:33:28,871] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Utils: /home/***/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar has been previously copied to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar
[2022-02-19 00:33:28,874] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Executor: Fetching file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.12.0.jar with timestamp 1645230806608
[2022-02-19 00:33:28,875] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Utils: /home/***/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.12.0.jar has been previously copied to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.datastax.oss_java-driver-mapper-runtime-4.12.0.jar
[2022-02-19 00:33:28,878] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Executor: Fetching file:///home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar with timestamp 1645230806608
[2022-02-19 00:33:28,879] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Utils: /home/***/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar has been previously copied to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/org.apache.commons_commons-lang3-3.10.jar
[2022-02-19 00:33:28,882] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Executor: Fetching file:///home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1645230806608
[2022-02-19 00:33:28,883] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Utils: /home/***/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar has been previously copied to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar
[2022-02-19 00:33:28,886] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Executor: Fetching file:///home/***/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1645230806608
[2022-02-19 00:33:28,886] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Utils: /home/***/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar has been previously copied to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/org.reactivestreams_reactive-streams-1.0.3.jar
[2022-02-19 00:33:28,889] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Executor: Fetching file:///home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1645230806608
[2022-02-19 00:33:28,889] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Utils: /home/***/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar has been previously copied to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.thoughtworks.paranamer_paranamer-2.8.jar
[2022-02-19 00:33:28,893] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Executor: Fetching file:///home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.1.0.jar with timestamp 1645230806608
[2022-02-19 00:33:28,895] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Utils: /home/***/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.1.0.jar has been previously copied to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.datastax.spark_spark-cassandra-connector_2.12-3.1.0.jar
[2022-02-19 00:33:28,898] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Executor: Fetching file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.12.0.jar with timestamp 1645230806608
[2022-02-19 00:33:28,899] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Utils: /home/***/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.12.0.jar has been previously copied to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.datastax.oss_java-driver-query-builder-4.12.0.jar
[2022-02-19 00:33:28,902] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Executor: Fetching file:///home/***/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1645230806608
[2022-02-19 00:33:28,906] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Utils: /home/***/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar has been previously copied to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/org.scala-lang_scala-reflect-2.12.11.jar
[2022-02-19 00:33:28,909] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Executor: Fetching file:///home/***/.ivy2/jars/com.typesafe_config-1.4.1.jar with timestamp 1645230806608
[2022-02-19 00:33:28,910] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Utils: /home/***/.ivy2/jars/com.typesafe_config-1.4.1.jar has been previously copied to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.typesafe_config-1.4.1.jar
[2022-02-19 00:33:28,913] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Executor: Fetching file:///home/***/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.12.0.jar with timestamp 1645230806608
[2022-02-19 00:33:28,921] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Utils: /home/***/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.12.0.jar has been previously copied to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.datastax.oss_java-driver-core-shaded-4.12.0.jar
[2022-02-19 00:33:28,924] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Executor: Fetching file:///home/***/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1645230806608
[2022-02-19 00:33:28,925] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Utils: /home/***/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar has been previously copied to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/org.hdrhistogram_HdrHistogram-2.1.12.jar
[2022-02-19 00:33:28,930] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Executor: Fetching spark://2e85b2b3b082:35165/jars/com.thoughtworks.paranamer_paranamer-2.8.jar with timestamp 1645230806608
[2022-02-19 00:33:28,974] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO TransportClientFactory: Successfully created connection to 2e85b2b3b082/172.20.0.6:35165 after 34 ms (0 ms spent in bootstraps)
[2022-02-19 00:33:28,982] {subprocess.py:89} INFO - 22/02/19 00:33:28 INFO Utils: Fetching spark://2e85b2b3b082:35165/jars/com.thoughtworks.paranamer_paranamer-2.8.jar to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/fetchFileTemp11681012951191419489.tmp
[2022-02-19 00:33:29,019] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Utils: /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/fetchFileTemp11681012951191419489.tmp has been previously copied to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.thoughtworks.paranamer_paranamer-2.8.jar
[2022-02-19 00:33:29,024] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Executor: Adding file:/tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.thoughtworks.paranamer_paranamer-2.8.jar to class loader
[2022-02-19 00:33:29,024] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Executor: Fetching spark://2e85b2b3b082:35165/jars/com.google.code.findbugs_jsr305-3.0.2.jar with timestamp 1645230806608
[2022-02-19 00:33:29,024] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Utils: Fetching spark://2e85b2b3b082:35165/jars/com.google.code.findbugs_jsr305-3.0.2.jar to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/fetchFileTemp983855108008295464.tmp
[2022-02-19 00:33:29,026] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Utils: /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/fetchFileTemp983855108008295464.tmp has been previously copied to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.google.code.findbugs_jsr305-3.0.2.jar
[2022-02-19 00:33:29,029] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Executor: Adding file:/tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.google.code.findbugs_jsr305-3.0.2.jar to class loader
[2022-02-19 00:33:29,029] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Executor: Fetching spark://2e85b2b3b082:35165/jars/com.datastax.oss_java-driver-mapper-runtime-4.12.0.jar with timestamp 1645230806608
[2022-02-19 00:33:29,030] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Utils: Fetching spark://2e85b2b3b082:35165/jars/com.datastax.oss_java-driver-mapper-runtime-4.12.0.jar to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/fetchFileTemp17484051949513950640.tmp
[2022-02-19 00:33:29,032] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Utils: /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/fetchFileTemp17484051949513950640.tmp has been previously copied to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.datastax.oss_java-driver-mapper-runtime-4.12.0.jar
[2022-02-19 00:33:29,036] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Executor: Adding file:/tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.datastax.oss_java-driver-mapper-runtime-4.12.0.jar to class loader
[2022-02-19 00:33:29,036] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Executor: Fetching spark://2e85b2b3b082:35165/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar with timestamp 1645230806608
[2022-02-19 00:33:29,036] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Utils: Fetching spark://2e85b2b3b082:35165/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/fetchFileTemp7943238113909764530.tmp
[2022-02-19 00:33:29,051] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Utils: /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/fetchFileTemp7943238113909764530.tmp has been previously copied to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar
[2022-02-19 00:33:29,055] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Executor: Adding file:/tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar to class loader
[2022-02-19 00:33:29,055] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Executor: Fetching spark://2e85b2b3b082:35165/jars/org.apache.commons_commons-lang3-3.10.jar with timestamp 1645230806608
[2022-02-19 00:33:29,055] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Utils: Fetching spark://2e85b2b3b082:35165/jars/org.apache.commons_commons-lang3-3.10.jar to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/fetchFileTemp17673684897072974858.tmp
[2022-02-19 00:33:29,059] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Utils: /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/fetchFileTemp17673684897072974858.tmp has been previously copied to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/org.apache.commons_commons-lang3-3.10.jar
[2022-02-19 00:33:29,062] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Executor: Adding file:/tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/org.apache.commons_commons-lang3-3.10.jar to class loader
[2022-02-19 00:33:29,062] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Executor: Fetching spark://2e85b2b3b082:35165/jars/com.datastax.oss_java-driver-query-builder-4.12.0.jar with timestamp 1645230806608
[2022-02-19 00:33:29,062] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Utils: Fetching spark://2e85b2b3b082:35165/jars/com.datastax.oss_java-driver-query-builder-4.12.0.jar to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/fetchFileTemp14206010126297619104.tmp
[2022-02-19 00:33:29,067] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Utils: /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/fetchFileTemp14206010126297619104.tmp has been previously copied to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.datastax.oss_java-driver-query-builder-4.12.0.jar
[2022-02-19 00:33:29,070] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Executor: Adding file:/tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.datastax.oss_java-driver-query-builder-4.12.0.jar to class loader
[2022-02-19 00:33:29,071] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Executor: Fetching spark://2e85b2b3b082:35165/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar with timestamp 1645230806608
[2022-02-19 00:33:29,071] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Utils: Fetching spark://2e85b2b3b082:35165/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/fetchFileTemp4971018049214608889.tmp
[2022-02-19 00:33:29,072] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Utils: /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/fetchFileTemp4971018049214608889.tmp has been previously copied to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.github.spotbugs_spotbugs-annotations-3.1.12.jar
[2022-02-19 00:33:29,075] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Executor: Adding file:/tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.github.spotbugs_spotbugs-annotations-3.1.12.jar to class loader
[2022-02-19 00:33:29,076] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Executor: Fetching spark://2e85b2b3b082:35165/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.1.0.jar with timestamp 1645230806608
[2022-02-19 00:33:29,076] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Utils: Fetching spark://2e85b2b3b082:35165/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.1.0.jar to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/fetchFileTemp11429849191324973869.tmp
[2022-02-19 00:33:29,084] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Utils: /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/fetchFileTemp11429849191324973869.tmp has been previously copied to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.datastax.spark_spark-cassandra-connector_2.12-3.1.0.jar
[2022-02-19 00:33:29,087] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Executor: Adding file:/tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.datastax.spark_spark-cassandra-connector_2.12-3.1.0.jar to class loader
[2022-02-19 00:33:29,088] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Executor: Fetching spark://2e85b2b3b082:35165/jars/org.scala-lang_scala-reflect-2.12.11.jar with timestamp 1645230806608
[2022-02-19 00:33:29,088] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Utils: Fetching spark://2e85b2b3b082:35165/jars/org.scala-lang_scala-reflect-2.12.11.jar to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/fetchFileTemp17555865233271081811.tmp
[2022-02-19 00:33:29,101] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Utils: /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/fetchFileTemp17555865233271081811.tmp has been previously copied to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/org.scala-lang_scala-reflect-2.12.11.jar
[2022-02-19 00:33:29,105] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Executor: Adding file:/tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/org.scala-lang_scala-reflect-2.12.11.jar to class loader
[2022-02-19 00:33:29,105] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Executor: Fetching spark://2e85b2b3b082:35165/jars/org.reactivestreams_reactive-streams-1.0.3.jar with timestamp 1645230806608
[2022-02-19 00:33:29,105] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Utils: Fetching spark://2e85b2b3b082:35165/jars/org.reactivestreams_reactive-streams-1.0.3.jar to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/fetchFileTemp12953752451329559885.tmp
[2022-02-19 00:33:29,107] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Utils: /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/fetchFileTemp12953752451329559885.tmp has been previously copied to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/org.reactivestreams_reactive-streams-1.0.3.jar
[2022-02-19 00:33:29,110] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Executor: Adding file:/tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/org.reactivestreams_reactive-streams-1.0.3.jar to class loader
[2022-02-19 00:33:29,111] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Executor: Fetching spark://2e85b2b3b082:35165/jars/org.slf4j_slf4j-api-1.7.26.jar with timestamp 1645230806608
[2022-02-19 00:33:29,111] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Utils: Fetching spark://2e85b2b3b082:35165/jars/org.slf4j_slf4j-api-1.7.26.jar to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/fetchFileTemp11004810967704601447.tmp
[2022-02-19 00:33:29,112] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Utils: /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/fetchFileTemp11004810967704601447.tmp has been previously copied to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/org.slf4j_slf4j-api-1.7.26.jar
[2022-02-19 00:33:29,115] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Executor: Adding file:/tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/org.slf4j_slf4j-api-1.7.26.jar to class loader
[2022-02-19 00:33:29,115] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Executor: Fetching spark://2e85b2b3b082:35165/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar with timestamp 1645230806608
[2022-02-19 00:33:29,116] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Utils: Fetching spark://2e85b2b3b082:35165/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/fetchFileTemp10237436214412557992.tmp
[2022-02-19 00:33:29,117] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Utils: /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/fetchFileTemp10237436214412557992.tmp has been previously copied to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar
[2022-02-19 00:33:29,120] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Executor: Adding file:/tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar to class loader
[2022-02-19 00:33:29,120] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Executor: Fetching spark://2e85b2b3b082:35165/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar with timestamp 1645230806608
[2022-02-19 00:33:29,121] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Utils: Fetching spark://2e85b2b3b082:35165/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/fetchFileTemp8975511430759069530.tmp
[2022-02-19 00:33:29,123] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Utils: /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/fetchFileTemp8975511430759069530.tmp has been previously copied to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/org.hdrhistogram_HdrHistogram-2.1.12.jar
[2022-02-19 00:33:29,126] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Executor: Adding file:/tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/org.hdrhistogram_HdrHistogram-2.1.12.jar to class loader
[2022-02-19 00:33:29,127] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Executor: Fetching spark://2e85b2b3b082:35165/jars/com.datastax.oss_native-protocol-1.5.0.jar with timestamp 1645230806608
[2022-02-19 00:33:29,127] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Utils: Fetching spark://2e85b2b3b082:35165/jars/com.datastax.oss_native-protocol-1.5.0.jar to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/fetchFileTemp6477002295206547560.tmp
[2022-02-19 00:33:29,130] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Utils: /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/fetchFileTemp6477002295206547560.tmp has been previously copied to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.datastax.oss_native-protocol-1.5.0.jar
[2022-02-19 00:33:29,133] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Executor: Adding file:/tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.datastax.oss_native-protocol-1.5.0.jar to class loader
[2022-02-19 00:33:29,133] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Executor: Fetching spark://2e85b2b3b082:35165/jars/com.typesafe_config-1.4.1.jar with timestamp 1645230806608
[2022-02-19 00:33:29,133] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Utils: Fetching spark://2e85b2b3b082:35165/jars/com.typesafe_config-1.4.1.jar to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/fetchFileTemp15416944845904514864.tmp
[2022-02-19 00:33:29,136] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Utils: /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/fetchFileTemp15416944845904514864.tmp has been previously copied to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.typesafe_config-1.4.1.jar
[2022-02-19 00:33:29,139] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Executor: Adding file:/tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.typesafe_config-1.4.1.jar to class loader
[2022-02-19 00:33:29,139] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Executor: Fetching spark://2e85b2b3b082:35165/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar with timestamp 1645230806608
[2022-02-19 00:33:29,139] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Utils: Fetching spark://2e85b2b3b082:35165/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/fetchFileTemp17217495411007218640.tmp
[2022-02-19 00:33:29,141] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Utils: /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/fetchFileTemp17217495411007218640.tmp has been previously copied to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/io.dropwizard.metrics_metrics-core-4.1.18.jar
[2022-02-19 00:33:29,144] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Executor: Adding file:/tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/io.dropwizard.metrics_metrics-core-4.1.18.jar to class loader
[2022-02-19 00:33:29,145] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Executor: Fetching spark://2e85b2b3b082:35165/jars/com.datastax.oss_java-driver-core-shaded-4.12.0.jar with timestamp 1645230806608
[2022-02-19 00:33:29,145] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Utils: Fetching spark://2e85b2b3b082:35165/jars/com.datastax.oss_java-driver-core-shaded-4.12.0.jar to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/fetchFileTemp8591296009163145730.tmp
[2022-02-19 00:33:29,172] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Utils: /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/fetchFileTemp8591296009163145730.tmp has been previously copied to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.datastax.oss_java-driver-core-shaded-4.12.0.jar
[2022-02-19 00:33:29,176] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Executor: Adding file:/tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.datastax.oss_java-driver-core-shaded-4.12.0.jar to class loader
[2022-02-19 00:33:29,176] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Executor: Fetching spark://2e85b2b3b082:35165/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.1.0.jar with timestamp 1645230806608
[2022-02-19 00:33:29,177] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Utils: Fetching spark://2e85b2b3b082:35165/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.1.0.jar to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/fetchFileTemp3079215201872762007.tmp
[2022-02-19 00:33:29,182] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Utils: /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/fetchFileTemp3079215201872762007.tmp has been previously copied to /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.1.0.jar
[2022-02-19 00:33:29,185] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Executor: Adding file:/tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/userFiles-2b6e22e9-eabf-4271-82e5-29531be745d4/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.1.0.jar to class loader
[2022-02-19 00:33:29,193] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36199.
[2022-02-19 00:33:29,193] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO NettyBlockTransferService: Server created on 2e85b2b3b082:36199
[2022-02-19 00:33:29,194] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2022-02-19 00:33:29,201] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 2e85b2b3b082, 36199, None)
[2022-02-19 00:33:29,206] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO BlockManagerMasterEndpoint: Registering block manager 2e85b2b3b082:36199 with 434.4 MiB RAM, BlockManagerId(driver, 2e85b2b3b082, 36199, None)
[2022-02-19 00:33:29,208] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 2e85b2b3b082, 36199, None)
[2022-02-19 00:33:29,210] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 2e85b2b3b082, 36199, None)
[2022-02-19 00:33:29,673] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2022-02-19 00:33:29,676] {subprocess.py:89} INFO - 22/02/19 00:33:29 INFO SharedState: Warehouse path is 'file:/tmp/***tmpvqnm7ppz/spark-warehouse'.
[2022-02-19 00:33:32,637] {subprocess.py:89} INFO - 22/02/19 00:33:32 INFO DefaultMavenCoordinates: DataStax Java driver for Apache Cassandra(R) (com.datastax.oss:java-driver-core-shaded) version 4.12.0
[2022-02-19 00:33:32,819] {subprocess.py:89} INFO - 22/02/19 00:33:32 INFO Native: Unable to load JNR native implementation. This could be normal if JNR is excluded from the classpath
[2022-02-19 00:33:32,820] {subprocess.py:89} INFO - java.lang.NoClassDefFoundError: jnr/posix/POSIXHandler
[2022-02-19 00:33:32,820] {subprocess.py:89} INFO - 	at com.datastax.oss.driver.internal.core.os.Native$LibcLoader.load(Native.java:42)
[2022-02-19 00:33:32,820] {subprocess.py:89} INFO - 	at com.datastax.oss.driver.internal.core.os.Native.<clinit>(Native.java:59)
[2022-02-19 00:33:32,820] {subprocess.py:89} INFO - 	at com.datastax.oss.driver.internal.core.time.Clock.getInstance(Clock.java:41)
[2022-02-19 00:33:32,820] {subprocess.py:89} INFO - 	at com.datastax.oss.driver.internal.core.time.MonotonicTimestampGenerator.buildClock(MonotonicTimestampGenerator.java:109)
[2022-02-19 00:33:32,820] {subprocess.py:89} INFO - 	at com.datastax.oss.driver.internal.core.time.MonotonicTimestampGenerator.<init>(MonotonicTimestampGenerator.java:43)
[2022-02-19 00:33:32,820] {subprocess.py:89} INFO - 	at com.datastax.oss.driver.internal.core.time.AtomicTimestampGenerator.<init>(AtomicTimestampGenerator.java:52)
[2022-02-19 00:33:32,820] {subprocess.py:89} INFO - 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2022-02-19 00:33:32,820] {subprocess.py:89} INFO - 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
[2022-02-19 00:33:32,820] {subprocess.py:89} INFO - 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[2022-02-19 00:33:32,821] {subprocess.py:89} INFO - 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
[2022-02-19 00:33:32,821] {subprocess.py:89} INFO - 	at com.datastax.oss.driver.internal.core.util.Reflection.buildFromConfig(Reflection.java:246)
[2022-02-19 00:33:32,821] {subprocess.py:89} INFO - 	at com.datastax.oss.driver.internal.core.util.Reflection.buildFromConfig(Reflection.java:108)
[2022-02-19 00:33:32,821] {subprocess.py:89} INFO - 	at com.datastax.oss.driver.internal.core.context.DefaultDriverContext.buildTimestampGenerator(DefaultDriverContext.java:370)
[2022-02-19 00:33:32,821] {subprocess.py:89} INFO - 	at com.datastax.oss.driver.internal.core.util.concurrent.LazyReference.get(LazyReference.java:55)
[2022-02-19 00:33:32,821] {subprocess.py:89} INFO - 	at com.datastax.oss.driver.internal.core.context.DefaultDriverContext.getTimestampGenerator(DefaultDriverContext.java:709)
[2022-02-19 00:33:32,821] {subprocess.py:89} INFO - 	at com.datastax.oss.driver.internal.core.session.DefaultSession$SingleThreaded.init(DefaultSession.java:349)
[2022-02-19 00:33:32,821] {subprocess.py:89} INFO - 	at com.datastax.oss.driver.internal.core.session.DefaultSession$SingleThreaded.access$1100(DefaultSession.java:300)
[2022-02-19 00:33:32,821] {subprocess.py:89} INFO - 	at com.datastax.oss.driver.internal.core.session.DefaultSession.lambda$init$0(DefaultSession.java:146)
[2022-02-19 00:33:32,821] {subprocess.py:89} INFO - 	at com.datastax.oss.driver.shaded.netty.util.concurrent.PromiseTask.runTask(PromiseTask.java:98)
[2022-02-19 00:33:32,822] {subprocess.py:89} INFO - 	at com.datastax.oss.driver.shaded.netty.util.concurrent.PromiseTask.run(PromiseTask.java:106)
[2022-02-19 00:33:32,822] {subprocess.py:89} INFO - 	at com.datastax.oss.driver.shaded.netty.channel.DefaultEventLoop.run(DefaultEventLoop.java:54)
[2022-02-19 00:33:32,822] {subprocess.py:89} INFO - 	at com.datastax.oss.driver.shaded.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
[2022-02-19 00:33:32,822] {subprocess.py:89} INFO - 	at com.datastax.oss.driver.shaded.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-02-19 00:33:32,822] {subprocess.py:89} INFO - 	at com.datastax.oss.driver.shaded.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-02-19 00:33:32,822] {subprocess.py:89} INFO - 	at java.base/java.lang.Thread.run(Thread.java:829)
[2022-02-19 00:33:32,822] {subprocess.py:89} INFO - Caused by: java.lang.ClassNotFoundException: jnr.posix.POSIXHandler
[2022-02-19 00:33:32,822] {subprocess.py:89} INFO - 	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)
[2022-02-19 00:33:32,822] {subprocess.py:89} INFO - 	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)
[2022-02-19 00:33:32,822] {subprocess.py:89} INFO - 	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)
[2022-02-19 00:33:32,822] {subprocess.py:89} INFO - 	... 25 more
[2022-02-19 00:33:32,823] {subprocess.py:89} INFO - 22/02/19 00:33:32 INFO Clock: Could not access native clock (see debug logs for details), falling back to Java system clock
[2022-02-19 00:33:33,810] {subprocess.py:89} INFO - 22/02/19 00:33:33 INFO CassandraConnector: Connected to Cassandra cluster.
[2022-02-19 00:33:35,226] {subprocess.py:89} INFO - 22/02/19 00:33:35 INFO V2ScanRelationPushDown:
[2022-02-19 00:33:35,227] {subprocess.py:89} INFO - Output: job_title#0, employee_id#1, employee_name#2, first_day#3, last_day#4
[2022-02-19 00:33:35,227] {subprocess.py:89} INFO - 
[2022-02-19 00:33:36,936] {subprocess.py:89} INFO - 22/02/19 00:33:36 INFO CodeGenerator: Code generated in 214.6054 ms
[2022-02-19 00:33:37,088] {subprocess.py:89} INFO - 22/02/19 00:33:37 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[2022-02-19 00:33:37,103] {subprocess.py:89} INFO - 22/02/19 00:33:37 INFO DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-02-19 00:33:37,104] {subprocess.py:89} INFO - 22/02/19 00:33:37 INFO DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)
[2022-02-19 00:33:37,104] {subprocess.py:89} INFO - 22/02/19 00:33:37 INFO DAGScheduler: Parents of final stage: List()
[2022-02-19 00:33:37,105] {subprocess.py:89} INFO - 22/02/19 00:33:37 INFO DAGScheduler: Missing parents: List()
[2022-02-19 00:33:37,109] {subprocess.py:89} INFO - 22/02/19 00:33:37 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-02-19 00:33:37,315] {subprocess.py:89} INFO - 22/02/19 00:33:37 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 17.8 KiB, free 434.4 MiB)
[2022-02-19 00:33:37,382] {subprocess.py:89} INFO - 22/02/19 00:33:37 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 8.5 KiB, free 434.4 MiB)
[2022-02-19 00:33:37,386] {subprocess.py:89} INFO - 22/02/19 00:33:37 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 2e85b2b3b082:36199 (size: 8.5 KiB, free: 434.4 MiB)
[2022-02-19 00:33:37,392] {subprocess.py:89} INFO - 22/02/19 00:33:37 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1427
[2022-02-19 00:33:37,410] {subprocess.py:89} INFO - 22/02/19 00:33:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-02-19 00:33:37,411] {subprocess.py:89} INFO - 22/02/19 00:33:37 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2022-02-19 00:33:37,502] {subprocess.py:89} INFO - 22/02/19 00:33:37 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (2e85b2b3b082, executor driver, partition 0, ANY, 8688 bytes) taskResourceAssignments Map()
[2022-02-19 00:33:37,520] {subprocess.py:89} INFO - 22/02/19 00:33:37 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2022-02-19 00:33:38,171] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1390 bytes result sent to driver
[2022-02-19 00:33:38,180] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 713 ms on 2e85b2b3b082 (executor driver) (1/1)
[2022-02-19 00:33:38,182] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2022-02-19 00:33:38,187] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 1.044 s
[2022-02-19 00:33:38,191] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-02-19 00:33:38,191] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2022-02-19 00:33:38,193] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO DAGScheduler: Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 1.104673 s
[2022-02-19 00:33:38,210] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[2022-02-19 00:33:38,211] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 4 output partitions
[2022-02-19 00:33:38,211] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)
[2022-02-19 00:33:38,211] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO DAGScheduler: Parents of final stage: List()
[2022-02-19 00:33:38,211] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO DAGScheduler: Missing parents: List()
[2022-02-19 00:33:38,213] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-02-19 00:33:38,218] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 17.8 KiB, free 434.4 MiB)
[2022-02-19 00:33:38,221] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.5 KiB, free 434.3 MiB)
[2022-02-19 00:33:38,222] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 2e85b2b3b082:36199 (size: 8.5 KiB, free: 434.4 MiB)
[2022-02-19 00:33:38,223] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1427
[2022-02-19 00:33:38,224] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(1, 2, 3, 4))
[2022-02-19 00:33:38,224] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO TaskSchedulerImpl: Adding task set 1.0 with 4 tasks resource profile 0
[2022-02-19 00:33:38,226] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (2e85b2b3b082, executor driver, partition 1, ANY, 7976 bytes) taskResourceAssignments Map()
[2022-02-19 00:33:38,227] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (2e85b2b3b082, executor driver, partition 2, ANY, 7019 bytes) taskResourceAssignments Map()
[2022-02-19 00:33:38,229] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (2e85b2b3b082, executor driver, partition 3, ANY, 7499 bytes) taskResourceAssignments Map()
[2022-02-19 00:33:38,230] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (2e85b2b3b082, executor driver, partition 4, ANY, 7736 bytes) taskResourceAssignments Map()
[2022-02-19 00:33:38,231] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2022-02-19 00:33:38,234] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
[2022-02-19 00:33:38,234] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
[2022-02-19 00:33:38,249] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)
[2022-02-19 00:33:38,366] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 1390 bytes result sent to driver
[2022-02-19 00:33:38,369] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 1390 bytes result sent to driver
[2022-02-19 00:33:38,378] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 152 ms on 2e85b2b3b082 (executor driver) (1/4)
[2022-02-19 00:33:38,382] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 154 ms on 2e85b2b3b082 (executor driver) (2/4)
[2022-02-19 00:33:38,414] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1390 bytes result sent to driver
[2022-02-19 00:33:38,419] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 194 ms on 2e85b2b3b082 (executor driver) (3/4)
[2022-02-19 00:33:38,469] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 1390 bytes result sent to driver
[2022-02-19 00:33:38,471] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 242 ms on 2e85b2b3b082 (executor driver) (4/4)
[2022-02-19 00:33:38,471] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2022-02-19 00:33:38,473] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.258 s
[2022-02-19 00:33:38,474] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-02-19 00:33:38,475] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2022-02-19 00:33:38,475] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 0.262669 s
[2022-02-19 00:33:38,485] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[2022-02-19 00:33:38,487] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO DAGScheduler: Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 14 output partitions
[2022-02-19 00:33:38,487] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO DAGScheduler: Final stage: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0)
[2022-02-19 00:33:38,487] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO DAGScheduler: Parents of final stage: List()
[2022-02-19 00:33:38,487] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO DAGScheduler: Missing parents: List()
[2022-02-19 00:33:38,489] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[3] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-02-19 00:33:38,495] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 17.8 KiB, free 434.3 MiB)
[2022-02-19 00:33:38,499] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 8.5 KiB, free 434.3 MiB)
[2022-02-19 00:33:38,500] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 2e85b2b3b082:36199 (size: 8.5 KiB, free: 434.4 MiB)
[2022-02-19 00:33:38,501] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1427
[2022-02-19 00:33:38,502] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO DAGScheduler: Submitting 14 missing tasks from ResultStage 2 (MapPartitionsRDD[3] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18))
[2022-02-19 00:33:38,502] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO TaskSchedulerImpl: Adding task set 2.0 with 14 tasks resource profile 0
[2022-02-19 00:33:38,504] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 5) (2e85b2b3b082, executor driver, partition 5, ANY, 7975 bytes) taskResourceAssignments Map()
[2022-02-19 00:33:38,506] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 6) (2e85b2b3b082, executor driver, partition 6, ANY, 7976 bytes) taskResourceAssignments Map()
[2022-02-19 00:33:38,507] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 7) (2e85b2b3b082, executor driver, partition 7, ANY, 7500 bytes) taskResourceAssignments Map()
[2022-02-19 00:33:38,513] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 8) (2e85b2b3b082, executor driver, partition 8, ANY, 7857 bytes) taskResourceAssignments Map()
[2022-02-19 00:33:38,513] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO TaskSetManager: Starting task 4.0 in stage 2.0 (TID 9) (2e85b2b3b082, executor driver, partition 9, ANY, 7502 bytes) taskResourceAssignments Map()
[2022-02-19 00:33:38,514] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO TaskSetManager: Starting task 5.0 in stage 2.0 (TID 10) (2e85b2b3b082, executor driver, partition 10, ANY, 7260 bytes) taskResourceAssignments Map()
[2022-02-19 00:33:38,514] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO TaskSetManager: Starting task 6.0 in stage 2.0 (TID 11) (2e85b2b3b082, executor driver, partition 11, ANY, 7977 bytes) taskResourceAssignments Map()
[2022-02-19 00:33:38,514] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO TaskSetManager: Starting task 7.0 in stage 2.0 (TID 12) (2e85b2b3b082, executor driver, partition 12, ANY, 7501 bytes) taskResourceAssignments Map()
[2022-02-19 00:33:38,514] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO Executor: Running task 1.0 in stage 2.0 (TID 6)
[2022-02-19 00:33:38,514] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO Executor: Running task 0.0 in stage 2.0 (TID 5)
[2022-02-19 00:33:38,530] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO Executor: Running task 2.0 in stage 2.0 (TID 7)
[2022-02-19 00:33:38,541] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO Executor: Running task 3.0 in stage 2.0 (TID 8)
[2022-02-19 00:33:38,550] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO Executor: Running task 4.0 in stage 2.0 (TID 9)
[2022-02-19 00:33:38,579] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO Executor: Running task 5.0 in stage 2.0 (TID 10)
[2022-02-19 00:33:38,614] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO Executor: Running task 6.0 in stage 2.0 (TID 11)
[2022-02-19 00:33:38,650] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO Executor: Running task 7.0 in stage 2.0 (TID 12)
[2022-02-19 00:33:38,708] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO Executor: Finished task 2.0 in stage 2.0 (TID 7). 1390 bytes result sent to driver
[2022-02-19 00:33:38,710] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO TaskSetManager: Starting task 8.0 in stage 2.0 (TID 13) (2e85b2b3b082, executor driver, partition 13, ANY, 7021 bytes) taskResourceAssignments Map()
[2022-02-19 00:33:38,717] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 7) in 204 ms on 2e85b2b3b082 (executor driver) (1/14)
[2022-02-19 00:33:38,721] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO Executor: Running task 8.0 in stage 2.0 (TID 13)
[2022-02-19 00:33:38,730] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO Executor: Finished task 4.0 in stage 2.0 (TID 9). 1390 bytes result sent to driver
[2022-02-19 00:33:38,732] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO Executor: Finished task 5.0 in stage 2.0 (TID 10). 1390 bytes result sent to driver
[2022-02-19 00:33:38,735] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO Executor: Finished task 1.0 in stage 2.0 (TID 6). 1390 bytes result sent to driver
[2022-02-19 00:33:38,738] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO TaskSetManager: Starting task 9.0 in stage 2.0 (TID 14) (2e85b2b3b082, executor driver, partition 14, ANY, 8807 bytes) taskResourceAssignments Map()
[2022-02-19 00:33:38,738] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO Executor: Running task 9.0 in stage 2.0 (TID 14)
[2022-02-19 00:33:38,740] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO TaskSetManager: Starting task 10.0 in stage 2.0 (TID 15) (2e85b2b3b082, executor driver, partition 15, ANY, 7618 bytes) taskResourceAssignments Map()
[2022-02-19 00:33:38,741] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO TaskSetManager: Finished task 4.0 in stage 2.0 (TID 9) in 233 ms on 2e85b2b3b082 (executor driver) (2/14)
[2022-02-19 00:33:38,741] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO Executor: Running task 10.0 in stage 2.0 (TID 15)
[2022-02-19 00:33:38,743] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO TaskSetManager: Starting task 11.0 in stage 2.0 (TID 16) (2e85b2b3b082, executor driver, partition 16, ANY, 8213 bytes) taskResourceAssignments Map()
[2022-02-19 00:33:38,744] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO Executor: Running task 11.0 in stage 2.0 (TID 16)
[2022-02-19 00:33:38,755] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 6) in 251 ms on 2e85b2b3b082 (executor driver) (3/14)
[2022-02-19 00:33:38,757] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO TaskSetManager: Finished task 5.0 in stage 2.0 (TID 10) in 248 ms on 2e85b2b3b082 (executor driver) (4/14)
[2022-02-19 00:33:38,791] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO Executor: Finished task 3.0 in stage 2.0 (TID 8). 1390 bytes result sent to driver
[2022-02-19 00:33:38,793] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO TaskSetManager: Starting task 12.0 in stage 2.0 (TID 17) (2e85b2b3b082, executor driver, partition 17, ANY, 7972 bytes) taskResourceAssignments Map()
[2022-02-19 00:33:38,796] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 8) in 286 ms on 2e85b2b3b082 (executor driver) (5/14)
[2022-02-19 00:33:38,796] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO Executor: Running task 12.0 in stage 2.0 (TID 17)
[2022-02-19 00:33:38,810] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO Executor: Finished task 0.0 in stage 2.0 (TID 5). 1390 bytes result sent to driver
[2022-02-19 00:33:38,813] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO TaskSetManager: Starting task 13.0 in stage 2.0 (TID 18) (2e85b2b3b082, executor driver, partition 18, ANY, 7974 bytes) taskResourceAssignments Map()
[2022-02-19 00:33:38,818] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO Executor: Running task 13.0 in stage 2.0 (TID 18)
[2022-02-19 00:33:38,821] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 5) in 311 ms on 2e85b2b3b082 (executor driver) (6/14)
[2022-02-19 00:33:38,827] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO Executor: Finished task 7.0 in stage 2.0 (TID 12). 1390 bytes result sent to driver
[2022-02-19 00:33:38,827] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO TaskSetManager: Finished task 7.0 in stage 2.0 (TID 12) in 314 ms on 2e85b2b3b082 (executor driver) (7/14)
[2022-02-19 00:33:38,839] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO Executor: Finished task 8.0 in stage 2.0 (TID 13). 1390 bytes result sent to driver
[2022-02-19 00:33:38,843] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO TaskSetManager: Finished task 8.0 in stage 2.0 (TID 13) in 133 ms on 2e85b2b3b082 (executor driver) (8/14)
[2022-02-19 00:33:38,887] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO Executor: Finished task 6.0 in stage 2.0 (TID 11). 1390 bytes result sent to driver
[2022-02-19 00:33:38,889] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO TaskSetManager: Finished task 6.0 in stage 2.0 (TID 11) in 378 ms on 2e85b2b3b082 (executor driver) (9/14)
[2022-02-19 00:33:38,893] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO Executor: Finished task 10.0 in stage 2.0 (TID 15). 1390 bytes result sent to driver
[2022-02-19 00:33:38,899] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO TaskSetManager: Finished task 10.0 in stage 2.0 (TID 15) in 159 ms on 2e85b2b3b082 (executor driver) (10/14)
[2022-02-19 00:33:38,940] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO Executor: Finished task 11.0 in stage 2.0 (TID 16). 1390 bytes result sent to driver
[2022-02-19 00:33:38,944] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO TaskSetManager: Finished task 11.0 in stage 2.0 (TID 16) in 199 ms on 2e85b2b3b082 (executor driver) (11/14)
[2022-02-19 00:33:38,952] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO Executor: Finished task 12.0 in stage 2.0 (TID 17). 1390 bytes result sent to driver
[2022-02-19 00:33:38,953] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO TaskSetManager: Finished task 12.0 in stage 2.0 (TID 17) in 162 ms on 2e85b2b3b082 (executor driver) (12/14)
[2022-02-19 00:33:38,975] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO Executor: Finished task 13.0 in stage 2.0 (TID 18). 1390 bytes result sent to driver
[2022-02-19 00:33:38,975] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO Executor: Finished task 9.0 in stage 2.0 (TID 14). 1390 bytes result sent to driver
[2022-02-19 00:33:38,979] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO TaskSetManager: Finished task 13.0 in stage 2.0 (TID 18) in 167 ms on 2e85b2b3b082 (executor driver) (13/14)
[2022-02-19 00:33:38,979] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO TaskSetManager: Finished task 9.0 in stage 2.0 (TID 14) in 245 ms on 2e85b2b3b082 (executor driver) (14/14)
[2022-02-19 00:33:38,979] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2022-02-19 00:33:38,981] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO DAGScheduler: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0) finished in 0.489 s
[2022-02-19 00:33:38,982] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-02-19 00:33:38,982] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2022-02-19 00:33:38,982] {subprocess.py:89} INFO - 22/02/19 00:33:38 INFO DAGScheduler: Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 0.495010 s
[2022-02-19 00:33:38,991] {subprocess.py:89} INFO - +---------+-----------+-------------+---------+--------+
[2022-02-19 00:33:38,991] {subprocess.py:89} INFO - |job_title|employee_id|employee_name|first_day|last_day|
[2022-02-19 00:33:38,991] {subprocess.py:89} INFO - +---------+-----------+-------------+---------+--------+
[2022-02-19 00:33:38,991] {subprocess.py:89} INFO - +---------+-----------+-------------+---------+--------+
[2022-02-19 00:33:38,991] {subprocess.py:89} INFO - 
[2022-02-19 00:33:39,110] {subprocess.py:89} INFO - 22/02/19 00:33:39 INFO InMemoryFileIndex: It took 20 ms to list leaf files for 1 paths.
[2022-02-19 00:33:39,181] {subprocess.py:89} INFO - 22/02/19 00:33:39 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2022-02-19 00:33:39,536] {subprocess.py:89} INFO - 22/02/19 00:33:39 INFO FileSourceStrategy: Pushed Filters:
[2022-02-19 00:33:39,537] {subprocess.py:89} INFO - 22/02/19 00:33:39 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#36, None)) > 0)
[2022-02-19 00:33:39,540] {subprocess.py:89} INFO - 22/02/19 00:33:39 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-02-19 00:33:39,598] {subprocess.py:89} INFO - 22/02/19 00:33:39 INFO CodeGenerator: Code generated in 20.740966 ms
[2022-02-19 00:33:39,625] {subprocess.py:89} INFO - 22/02/19 00:33:39 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 199.8 KiB, free 434.1 MiB)
[2022-02-19 00:33:39,664] {subprocess.py:89} INFO - 22/02/19 00:33:39 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 35.8 KiB, free 434.1 MiB)
[2022-02-19 00:33:39,665] {subprocess.py:89} INFO - 22/02/19 00:33:39 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 2e85b2b3b082:36199 (size: 35.8 KiB, free: 434.3 MiB)
[2022-02-19 00:33:39,666] {subprocess.py:89} INFO - 22/02/19 00:33:39 INFO SparkContext: Created broadcast 3 from load at NativeMethodAccessorImpl.java:0
[2022-02-19 00:33:39,676] {subprocess.py:89} INFO - 22/02/19 00:33:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2022-02-19 00:33:39,747] {subprocess.py:89} INFO - 22/02/19 00:33:39 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2022-02-19 00:33:39,748] {subprocess.py:89} INFO - 22/02/19 00:33:39 INFO DAGScheduler: Got job 3 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2022-02-19 00:33:39,748] {subprocess.py:89} INFO - 22/02/19 00:33:39 INFO DAGScheduler: Final stage: ResultStage 3 (load at NativeMethodAccessorImpl.java:0)
[2022-02-19 00:33:39,748] {subprocess.py:89} INFO - 22/02/19 00:33:39 INFO DAGScheduler: Parents of final stage: List()
[2022-02-19 00:33:39,748] {subprocess.py:89} INFO - 22/02/19 00:33:39 INFO DAGScheduler: Missing parents: List()
[2022-02-19 00:33:39,750] {subprocess.py:89} INFO - 22/02/19 00:33:39 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[7] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-02-19 00:33:39,768] {subprocess.py:89} INFO - 22/02/19 00:33:39 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 11.6 KiB, free 434.1 MiB)
[2022-02-19 00:33:39,773] {subprocess.py:89} INFO - 22/02/19 00:33:39 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 434.1 MiB)
[2022-02-19 00:33:39,774] {subprocess.py:89} INFO - 22/02/19 00:33:39 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 2e85b2b3b082:36199 (size: 5.8 KiB, free: 434.3 MiB)
[2022-02-19 00:33:39,775] {subprocess.py:89} INFO - 22/02/19 00:33:39 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1427
[2022-02-19 00:33:39,776] {subprocess.py:89} INFO - 22/02/19 00:33:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2022-02-19 00:33:39,776] {subprocess.py:89} INFO - 22/02/19 00:33:39 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2022-02-19 00:33:39,781] {subprocess.py:89} INFO - 22/02/19 00:33:39 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 19) (2e85b2b3b082, executor driver, partition 0, PROCESS_LOCAL, 4889 bytes) taskResourceAssignments Map()
[2022-02-19 00:33:39,782] {subprocess.py:89} INFO - 22/02/19 00:33:39 INFO Executor: Running task 0.0 in stage 3.0 (TID 19)
[2022-02-19 00:33:39,823] {subprocess.py:89} INFO - 22/02/19 00:33:39 INFO FileScanRDD: Reading File path: file:///usr/local/spark/data/previous_employees_by_job_title.txt, range: 0-4194304, partition values: [empty row]
[2022-02-19 00:33:39,854] {subprocess.py:89} INFO - 22/02/19 00:33:39 INFO CodeGenerator: Code generated in 24.11167 ms
[2022-02-19 00:33:39,892] {subprocess.py:89} INFO - 22/02/19 00:33:39 INFO Executor: Finished task 0.0 in stage 3.0 (TID 19). 1557 bytes result sent to driver
[2022-02-19 00:33:39,895] {subprocess.py:89} INFO - 22/02/19 00:33:39 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 19) in 118 ms on 2e85b2b3b082 (executor driver) (1/1)
[2022-02-19 00:33:39,896] {subprocess.py:89} INFO - 22/02/19 00:33:39 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2022-02-19 00:33:39,899] {subprocess.py:89} INFO - 22/02/19 00:33:39 INFO DAGScheduler: ResultStage 3 (load at NativeMethodAccessorImpl.java:0) finished in 0.144 s
[2022-02-19 00:33:39,899] {subprocess.py:89} INFO - 22/02/19 00:33:39 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-02-19 00:33:39,899] {subprocess.py:89} INFO - 22/02/19 00:33:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2022-02-19 00:33:39,899] {subprocess.py:89} INFO - 22/02/19 00:33:39 INFO DAGScheduler: Job 3 finished: load at NativeMethodAccessorImpl.java:0, took 0.152072 s
[2022-02-19 00:33:39,923] {subprocess.py:89} INFO - 22/02/19 00:33:39 INFO CodeGenerator: Code generated in 12.436826 ms
[2022-02-19 00:33:39,976] {subprocess.py:89} INFO - 22/02/19 00:33:39 INFO FileSourceStrategy: Pushed Filters:
[2022-02-19 00:33:39,976] {subprocess.py:89} INFO - 22/02/19 00:33:39 INFO FileSourceStrategy: Post-Scan Filters:
[2022-02-19 00:33:39,976] {subprocess.py:89} INFO - 22/02/19 00:33:39 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2022-02-19 00:33:39,985] {subprocess.py:89} INFO - 22/02/19 00:33:39 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 199.8 KiB, free 433.9 MiB)
[2022-02-19 00:33:40,002] {subprocess.py:89} INFO - 22/02/19 00:33:40 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 35.8 KiB, free 433.8 MiB)
[2022-02-19 00:33:40,002] {subprocess.py:89} INFO - 22/02/19 00:33:40 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 2e85b2b3b082:36199 (size: 35.8 KiB, free: 434.3 MiB)
[2022-02-19 00:33:40,003] {subprocess.py:89} INFO - 22/02/19 00:33:40 INFO SparkContext: Created broadcast 5 from load at NativeMethodAccessorImpl.java:0
[2022-02-19 00:33:40,003] {subprocess.py:89} INFO - 22/02/19 00:33:40 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2022-02-19 00:33:40,248] {subprocess.py:89} INFO - 22/02/19 00:33:40 INFO FileSourceStrategy: Pushed Filters:
[2022-02-19 00:33:40,249] {subprocess.py:89} INFO - 22/02/19 00:33:40 INFO FileSourceStrategy: Post-Scan Filters:
[2022-02-19 00:33:40,249] {subprocess.py:89} INFO - 22/02/19 00:33:40 INFO FileSourceStrategy: Output Data Schema: struct<job_title: string, employee_id: string, employee_name: string, first_day: string, last_day: string ... 3 more fields>
[2022-02-19 00:33:40,266] {subprocess.py:89} INFO - 22/02/19 00:33:40 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 199.7 KiB, free 433.7 MiB)
[2022-02-19 00:33:40,282] {subprocess.py:89} INFO - 22/02/19 00:33:40 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 433.6 MiB)
[2022-02-19 00:33:40,283] {subprocess.py:89} INFO - 22/02/19 00:33:40 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 2e85b2b3b082:36199 (size: 35.9 KiB, free: 434.3 MiB)
[2022-02-19 00:33:40,284] {subprocess.py:89} INFO - 22/02/19 00:33:40 INFO SparkContext: Created broadcast 6 from save at NativeMethodAccessorImpl.java:0
[2022-02-19 00:33:40,288] {subprocess.py:89} INFO - 22/02/19 00:33:40 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2022-02-19 00:33:40,295] {subprocess.py:89} INFO - 22/02/19 00:33:40 INFO AppendDataExec: Start processing data source write support: CassandraBulkWrite(org.apache.spark.sql.SparkSession@693802a4,com.datastax.spark.connector.cql.CassandraConnector@5fffd91,TableDef(test,previous_employees_by_job_title,ArrayBuffer(ColumnDef(job_title,PartitionKeyColumn,VarCharType)),ArrayBuffer(ColumnDef(employee_id,ClusteringColumn(0,ASC),UUIDType)),Stream(ColumnDef(employee_name,RegularColumn,VarCharType), ColumnDef(first_day,RegularColumn,TimestampType), ColumnDef(last_day,RegularColumn,TimestampType)),Stream(),false,false,Map()),WriteConf(BytesInBatch(1024),1000,Partition,LOCAL_QUORUM,false,false,5,None,TTLOption(DefaultValue),TimestampOption(DefaultValue),true,None),StructType(StructField(job_title,StringType,true), StructField(employee_id,StringType,true), StructField(employee_name,StringType,true), StructField(first_day,StringType,true), StructField(last_day,StringType,true)),org.apache.spark.SparkConf@3b335fc9). The input RDD has 3 partitions.
[2022-02-19 00:33:40,312] {subprocess.py:89} INFO - 22/02/19 00:33:40 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
[2022-02-19 00:33:40,313] {subprocess.py:89} INFO - 22/02/19 00:33:40 INFO DAGScheduler: Got job 4 (save at NativeMethodAccessorImpl.java:0) with 3 output partitions
[2022-02-19 00:33:40,313] {subprocess.py:89} INFO - 22/02/19 00:33:40 INFO DAGScheduler: Final stage: ResultStage 4 (save at NativeMethodAccessorImpl.java:0)
[2022-02-19 00:33:40,314] {subprocess.py:89} INFO - 22/02/19 00:33:40 INFO DAGScheduler: Parents of final stage: List()
[2022-02-19 00:33:40,314] {subprocess.py:89} INFO - 22/02/19 00:33:40 INFO DAGScheduler: Missing parents: List()
[2022-02-19 00:33:40,314] {subprocess.py:89} INFO - 22/02/19 00:33:40 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[15] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-02-19 00:33:40,320] {subprocess.py:89} INFO - 22/02/19 00:33:40 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 16.5 KiB, free 433.6 MiB)
[2022-02-19 00:33:40,324] {subprocess.py:89} INFO - 22/02/19 00:33:40 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 8.6 KiB, free 433.6 MiB)
[2022-02-19 00:33:40,326] {subprocess.py:89} INFO - 22/02/19 00:33:40 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 2e85b2b3b082:36199 (size: 8.6 KiB, free: 434.3 MiB)
[2022-02-19 00:33:40,327] {subprocess.py:89} INFO - 22/02/19 00:33:40 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1427
[2022-02-19 00:33:40,327] {subprocess.py:89} INFO - 22/02/19 00:33:40 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 4 (MapPartitionsRDD[15] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))
[2022-02-19 00:33:40,328] {subprocess.py:89} INFO - 22/02/19 00:33:40 INFO TaskSchedulerImpl: Adding task set 4.0 with 3 tasks resource profile 0
[2022-02-19 00:33:40,329] {subprocess.py:89} INFO - 22/02/19 00:33:40 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 20) (2e85b2b3b082, executor driver, partition 0, PROCESS_LOCAL, 4889 bytes) taskResourceAssignments Map()
[2022-02-19 00:33:40,331] {subprocess.py:89} INFO - 22/02/19 00:33:40 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 21) (2e85b2b3b082, executor driver, partition 1, PROCESS_LOCAL, 4889 bytes) taskResourceAssignments Map()
[2022-02-19 00:33:40,331] {subprocess.py:89} INFO - 22/02/19 00:33:40 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 22) (2e85b2b3b082, executor driver, partition 2, PROCESS_LOCAL, 4889 bytes) taskResourceAssignments Map()
[2022-02-19 00:33:40,331] {subprocess.py:89} INFO - 22/02/19 00:33:40 INFO Executor: Running task 0.0 in stage 4.0 (TID 20)
[2022-02-19 00:33:40,331] {subprocess.py:89} INFO - 22/02/19 00:33:40 INFO Executor: Running task 1.0 in stage 4.0 (TID 21)
[2022-02-19 00:33:40,331] {subprocess.py:89} INFO - 22/02/19 00:33:40 INFO Executor: Running task 2.0 in stage 4.0 (TID 22)
[2022-02-19 00:33:40,435] {subprocess.py:89} INFO - 22/02/19 00:33:40 INFO FileScanRDD: Reading File path: file:///usr/local/spark/data/previous_employees_by_job_title.txt, range: 0-4194304, partition values: [empty row]
[2022-02-19 00:33:40,435] {subprocess.py:89} INFO - 22/02/19 00:33:40 INFO FileScanRDD: Reading File path: file:///usr/local/spark/data/previous_employees_by_job_title.txt, range: 4194304-8388608, partition values: [empty row]
[2022-02-19 00:33:40,436] {subprocess.py:89} INFO - 22/02/19 00:33:40 INFO FileScanRDD: Reading File path: file:///usr/local/spark/data/previous_employees_by_job_title.txt, range: 8388608-10679070, partition values: [empty row]
[2022-02-19 00:33:40,461] {subprocess.py:89} INFO - 22/02/19 00:33:40 INFO CodeGenerator: Code generated in 19.602983 ms
[2022-02-19 00:33:42,556] {subprocess.py:89} INFO - 22/02/19 00:33:42 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 2e85b2b3b082:36199 in memory (size: 8.5 KiB, free: 434.3 MiB)
[2022-02-19 00:33:42,637] {subprocess.py:89} INFO - 22/02/19 00:33:42 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 2e85b2b3b082:36199 in memory (size: 5.8 KiB, free: 434.3 MiB)
[2022-02-19 00:33:42,757] {subprocess.py:89} INFO - 22/02/19 00:33:42 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 2e85b2b3b082:36199 in memory (size: 8.5 KiB, free: 434.3 MiB)
[2022-02-19 00:33:42,878] {subprocess.py:89} INFO - 22/02/19 00:33:42 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 2e85b2b3b082:36199 in memory (size: 8.5 KiB, free: 434.3 MiB)
[2022-02-19 00:33:42,910] {subprocess.py:89} INFO - 22/02/19 00:33:42 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 2e85b2b3b082:36199 in memory (size: 35.8 KiB, free: 434.3 MiB)
[2022-02-19 00:33:42,948] {subprocess.py:89} INFO - 22/02/19 00:33:42 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 2e85b2b3b082:36199 in memory (size: 35.8 KiB, free: 434.4 MiB)
[2022-02-19 00:33:47,160] {subprocess.py:89} INFO - 22/02/19 00:33:47 INFO DataWritingSparkTask: Commit authorized for partition 2 (task 22, attempt 0, stage 4.0)
[2022-02-19 00:33:47,287] {subprocess.py:89} INFO - 22/02/19 00:33:47 INFO DataWritingSparkTask: Committed partition 2 (task 22, attempt 0, stage 4.0)
[2022-02-19 00:33:47,295] {subprocess.py:89} INFO - 22/02/19 00:33:47 INFO Executor: Finished task 2.0 in stage 4.0 (TID 22). 1507 bytes result sent to driver
[2022-02-19 00:33:47,297] {subprocess.py:89} INFO - 22/02/19 00:33:47 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 22) in 6967 ms on 2e85b2b3b082 (executor driver) (1/3)
[2022-02-19 00:33:48,984] {subprocess.py:89} INFO - 22/02/19 00:33:48 INFO DataWritingSparkTask: Commit authorized for partition 1 (task 21, attempt 0, stage 4.0)
[2022-02-19 00:33:49,020] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 20, attempt 0, stage 4.0)
[2022-02-19 00:33:49,046] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO DataWritingSparkTask: Committed partition 1 (task 21, attempt 0, stage 4.0)
[2022-02-19 00:33:49,054] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO Executor: Finished task 1.0 in stage 4.0 (TID 21). 1464 bytes result sent to driver
[2022-02-19 00:33:49,056] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 21) in 8726 ms on 2e85b2b3b082 (executor driver) (2/3)
[2022-02-19 00:33:49,075] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO DataWritingSparkTask: Committed partition 0 (task 20, attempt 0, stage 4.0)
[2022-02-19 00:33:49,077] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO Executor: Finished task 0.0 in stage 4.0 (TID 20). 1464 bytes result sent to driver
[2022-02-19 00:33:49,077] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 20) in 8748 ms on 2e85b2b3b082 (executor driver) (3/3)
[2022-02-19 00:33:49,078] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2022-02-19 00:33:49,079] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO DAGScheduler: ResultStage 4 (save at NativeMethodAccessorImpl.java:0) finished in 8.763 s
[2022-02-19 00:33:49,079] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-02-19 00:33:49,079] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2022-02-19 00:33:49,079] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO DAGScheduler: Job 4 finished: save at NativeMethodAccessorImpl.java:0, took 8.766947 s
[2022-02-19 00:33:49,080] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO AppendDataExec: Data source write support CassandraBulkWrite(org.apache.spark.sql.SparkSession@693802a4,com.datastax.spark.connector.cql.CassandraConnector@5fffd91,TableDef(test,previous_employees_by_job_title,ArrayBuffer(ColumnDef(job_title,PartitionKeyColumn,VarCharType)),ArrayBuffer(ColumnDef(employee_id,ClusteringColumn(0,ASC),UUIDType)),Stream(ColumnDef(employee_name,RegularColumn,VarCharType), ColumnDef(first_day,RegularColumn,TimestampType), ColumnDef(last_day,RegularColumn,TimestampType)),Stream(),false,false,Map()),WriteConf(BytesInBatch(1024),1000,Partition,LOCAL_QUORUM,false,false,5,None,TTLOption(DefaultValue),TimestampOption(DefaultValue),true,None),StructType(StructField(job_title,StringType,true), StructField(employee_id,StringType,true), StructField(employee_name,StringType,true), StructField(first_day,StringType,true), StructField(last_day,StringType,true)),org.apache.spark.SparkConf@3b335fc9) is committing.
[2022-02-19 00:33:49,081] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO AppendDataExec: Data source write support CassandraBulkWrite(org.apache.spark.sql.SparkSession@693802a4,com.datastax.spark.connector.cql.CassandraConnector@5fffd91,TableDef(test,previous_employees_by_job_title,ArrayBuffer(ColumnDef(job_title,PartitionKeyColumn,VarCharType)),ArrayBuffer(ColumnDef(employee_id,ClusteringColumn(0,ASC),UUIDType)),Stream(ColumnDef(employee_name,RegularColumn,VarCharType), ColumnDef(first_day,RegularColumn,TimestampType), ColumnDef(last_day,RegularColumn,TimestampType)),Stream(),false,false,Map()),WriteConf(BytesInBatch(1024),1000,Partition,LOCAL_QUORUM,false,false,5,None,TTLOption(DefaultValue),TimestampOption(DefaultValue),true,None),StructType(StructField(job_title,StringType,true), StructField(employee_id,StringType,true), StructField(employee_name,StringType,true), StructField(first_day,StringType,true), StructField(last_day,StringType,true)),org.apache.spark.SparkConf@3b335fc9) committed.
[2022-02-19 00:33:49,339] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO V2ScanRelationPushDown:
[2022-02-19 00:33:49,339] {subprocess.py:89} INFO - Output: job_title#83, employee_id#84, employee_name#85, first_day#86, last_day#87
[2022-02-19 00:33:49,339] {subprocess.py:89} INFO - 
[2022-02-19 00:33:49,461] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO CodeGenerator: Code generated in 26.090458 ms
[2022-02-19 00:33:49,470] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO AppendDataExec: Start processing data source write support: CassandraBulkWrite(org.apache.spark.sql.SparkSession@693802a4,com.datastax.spark.connector.cql.CassandraConnector@332fa6c4,TableDef(test,days_worked_by_previous_employees_by_job_title,ArrayBuffer(ColumnDef(job_title,PartitionKeyColumn,VarCharType)),ArrayBuffer(ColumnDef(employee_id,ClusteringColumn(0,ASC),UUIDType)),Stream(ColumnDef(days_worked,RegularColumn,IntType), ColumnDef(employee_name,RegularColumn,VarCharType)),Stream(),false,false,Map()),WriteConf(BytesInBatch(1024),1000,Partition,LOCAL_QUORUM,false,false,5,None,TTLOption(DefaultValue),TimestampOption(DefaultValue),true,None),StructType(StructField(job_title,StringType,false), StructField(employee_id,StringType,true), StructField(employee_name,StringType,true), StructField(days_worked,IntegerType,true)),org.apache.spark.SparkConf@6de20544). The input RDD has 19 partitions.
[2022-02-19 00:33:49,474] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
[2022-02-19 00:33:49,476] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO DAGScheduler: Got job 5 (save at NativeMethodAccessorImpl.java:0) with 19 output partitions
[2022-02-19 00:33:49,476] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO DAGScheduler: Final stage: ResultStage 5 (save at NativeMethodAccessorImpl.java:0)
[2022-02-19 00:33:49,476] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO DAGScheduler: Parents of final stage: List()
[2022-02-19 00:33:49,476] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO DAGScheduler: Missing parents: List()
[2022-02-19 00:33:49,477] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[18] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
[2022-02-19 00:33:49,483] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 19.5 KiB, free 434.1 MiB)
[2022-02-19 00:33:49,485] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 9.2 KiB, free 434.1 MiB)
[2022-02-19 00:33:49,486] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 2e85b2b3b082:36199 (size: 9.2 KiB, free: 434.3 MiB)
[2022-02-19 00:33:49,486] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1427
[2022-02-19 00:33:49,487] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO DAGScheduler: Submitting 19 missing tasks from ResultStage 5 (MapPartitionsRDD[18] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2022-02-19 00:33:49,487] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO TaskSchedulerImpl: Adding task set 5.0 with 19 tasks resource profile 0
[2022-02-19 00:33:49,491] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 23) (2e85b2b3b082, executor driver, partition 0, ANY, 7500 bytes) taskResourceAssignments Map()
[2022-02-19 00:33:49,495] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 24) (2e85b2b3b082, executor driver, partition 1, ANY, 7972 bytes) taskResourceAssignments Map()
[2022-02-19 00:33:49,496] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO TaskSetManager: Starting task 2.0 in stage 5.0 (TID 25) (2e85b2b3b082, executor driver, partition 2, ANY, 8213 bytes) taskResourceAssignments Map()
[2022-02-19 00:33:49,496] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO TaskSetManager: Starting task 3.0 in stage 5.0 (TID 26) (2e85b2b3b082, executor driver, partition 3, ANY, 7857 bytes) taskResourceAssignments Map()
[2022-02-19 00:33:49,497] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO TaskSetManager: Starting task 4.0 in stage 5.0 (TID 27) (2e85b2b3b082, executor driver, partition 4, ANY, 7736 bytes) taskResourceAssignments Map()
[2022-02-19 00:33:49,498] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO TaskSetManager: Starting task 5.0 in stage 5.0 (TID 28) (2e85b2b3b082, executor driver, partition 5, ANY, 7021 bytes) taskResourceAssignments Map()
[2022-02-19 00:33:49,499] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO TaskSetManager: Starting task 6.0 in stage 5.0 (TID 29) (2e85b2b3b082, executor driver, partition 6, ANY, 7502 bytes) taskResourceAssignments Map()
[2022-02-19 00:33:49,499] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO TaskSetManager: Starting task 7.0 in stage 5.0 (TID 30) (2e85b2b3b082, executor driver, partition 7, ANY, 7618 bytes) taskResourceAssignments Map()
[2022-02-19 00:33:49,500] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO Executor: Running task 0.0 in stage 5.0 (TID 23)
[2022-02-19 00:33:49,501] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO Executor: Running task 1.0 in stage 5.0 (TID 24)
[2022-02-19 00:33:49,501] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO Executor: Running task 4.0 in stage 5.0 (TID 27)
[2022-02-19 00:33:49,501] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO Executor: Running task 2.0 in stage 5.0 (TID 25)
[2022-02-19 00:33:49,501] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO Executor: Running task 5.0 in stage 5.0 (TID 28)
[2022-02-19 00:33:49,501] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO Executor: Running task 3.0 in stage 5.0 (TID 26)
[2022-02-19 00:33:49,507] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO Executor: Running task 6.0 in stage 5.0 (TID 29)
[2022-02-19 00:33:49,514] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO Executor: Running task 7.0 in stage 5.0 (TID 30)
[2022-02-19 00:33:49,834] {subprocess.py:89} INFO - 22/02/19 00:33:49 INFO CodeGenerator: Code generated in 17.441393 ms
[2022-02-19 00:33:50,071] {subprocess.py:89} INFO - 22/02/19 00:33:50 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 2e85b2b3b082:36199 in memory (size: 35.9 KiB, free: 434.4 MiB)
[2022-02-19 00:33:50,085] {subprocess.py:89} INFO - 22/02/19 00:33:50 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 2e85b2b3b082:36199 in memory (size: 8.6 KiB, free: 434.4 MiB)
[2022-02-19 00:33:50,671] {subprocess.py:89} INFO - 22/02/19 00:33:50 INFO DataWritingSparkTask: Commit authorized for partition 1 (task 24, attempt 0, stage 5.0)
[2022-02-19 00:33:50,679] {subprocess.py:89} INFO - 22/02/19 00:33:50 INFO DataWritingSparkTask: Committed partition 1 (task 24, attempt 0, stage 5.0)
[2022-02-19 00:33:50,687] {subprocess.py:89} INFO - 22/02/19 00:33:50 INFO Executor: Finished task 1.0 in stage 5.0 (TID 24). 1493 bytes result sent to driver
[2022-02-19 00:33:50,689] {subprocess.py:89} INFO - 22/02/19 00:33:50 INFO TaskSetManager: Starting task 8.0 in stage 5.0 (TID 31) (2e85b2b3b082, executor driver, partition 8, ANY, 7974 bytes) taskResourceAssignments Map()
[2022-02-19 00:33:50,689] {subprocess.py:89} INFO - 22/02/19 00:33:50 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 24) in 1198 ms on 2e85b2b3b082 (executor driver) (1/19)
[2022-02-19 00:33:50,689] {subprocess.py:89} INFO - 22/02/19 00:33:50 INFO Executor: Running task 8.0 in stage 5.0 (TID 31)
[2022-02-19 00:33:50,785] {subprocess.py:89} INFO - 22/02/19 00:33:50 INFO DataWritingSparkTask: Commit authorized for partition 5 (task 28, attempt 0, stage 5.0)
[2022-02-19 00:33:50,795] {subprocess.py:89} INFO - 22/02/19 00:33:50 INFO DataWritingSparkTask: Committed partition 5 (task 28, attempt 0, stage 5.0)
[2022-02-19 00:33:50,796] {subprocess.py:89} INFO - 22/02/19 00:33:50 INFO Executor: Finished task 5.0 in stage 5.0 (TID 28). 1493 bytes result sent to driver
[2022-02-19 00:33:50,798] {subprocess.py:89} INFO - 22/02/19 00:33:50 INFO TaskSetManager: Starting task 9.0 in stage 5.0 (TID 32) (2e85b2b3b082, executor driver, partition 9, ANY, 8688 bytes) taskResourceAssignments Map()
[2022-02-19 00:33:50,798] {subprocess.py:89} INFO - 22/02/19 00:33:50 INFO Executor: Running task 9.0 in stage 5.0 (TID 32)
[2022-02-19 00:33:50,802] {subprocess.py:89} INFO - 22/02/19 00:33:50 INFO TaskSetManager: Finished task 5.0 in stage 5.0 (TID 28) in 1304 ms on 2e85b2b3b082 (executor driver) (2/19)
[2022-02-19 00:33:50,811] {subprocess.py:89} INFO - 22/02/19 00:33:50 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 23, attempt 0, stage 5.0)
[2022-02-19 00:33:50,820] {subprocess.py:89} INFO - 22/02/19 00:33:50 INFO DataWritingSparkTask: Committed partition 0 (task 23, attempt 0, stage 5.0)
[2022-02-19 00:33:50,822] {subprocess.py:89} INFO - 22/02/19 00:33:50 INFO Executor: Finished task 0.0 in stage 5.0 (TID 23). 1493 bytes result sent to driver
[2022-02-19 00:33:50,823] {subprocess.py:89} INFO - 22/02/19 00:33:50 INFO TaskSetManager: Starting task 10.0 in stage 5.0 (TID 33) (2e85b2b3b082, executor driver, partition 10, ANY, 7019 bytes) taskResourceAssignments Map()
[2022-02-19 00:33:50,823] {subprocess.py:89} INFO - 22/02/19 00:33:50 INFO Executor: Running task 10.0 in stage 5.0 (TID 33)
[2022-02-19 00:33:50,824] {subprocess.py:89} INFO - 22/02/19 00:33:50 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 23) in 1334 ms on 2e85b2b3b082 (executor driver) (3/19)
[2022-02-19 00:33:51,049] {subprocess.py:89} INFO - 22/02/19 00:33:51 INFO DataWritingSparkTask: Commit authorized for partition 6 (task 29, attempt 0, stage 5.0)
[2022-02-19 00:33:51,077] {subprocess.py:89} INFO - 22/02/19 00:33:51 INFO DataWritingSparkTask: Committed partition 6 (task 29, attempt 0, stage 5.0)
[2022-02-19 00:33:51,084] {subprocess.py:89} INFO - 22/02/19 00:33:51 INFO Executor: Finished task 6.0 in stage 5.0 (TID 29). 1493 bytes result sent to driver
[2022-02-19 00:33:51,085] {subprocess.py:89} INFO - 22/02/19 00:33:51 INFO TaskSetManager: Starting task 11.0 in stage 5.0 (TID 34) (2e85b2b3b082, executor driver, partition 11, ANY, 7975 bytes) taskResourceAssignments Map()
[2022-02-19 00:33:51,085] {subprocess.py:89} INFO - 22/02/19 00:33:51 INFO TaskSetManager: Finished task 6.0 in stage 5.0 (TID 29) in 1581 ms on 2e85b2b3b082 (executor driver) (4/19)
[2022-02-19 00:33:51,085] {subprocess.py:89} INFO - 22/02/19 00:33:51 INFO Executor: Running task 11.0 in stage 5.0 (TID 34)
[2022-02-19 00:33:51,190] {subprocess.py:89} INFO - 22/02/19 00:33:51 INFO DataWritingSparkTask: Commit authorized for partition 2 (task 25, attempt 0, stage 5.0)
[2022-02-19 00:33:51,203] {subprocess.py:89} INFO - 22/02/19 00:33:51 INFO DataWritingSparkTask: Committed partition 2 (task 25, attempt 0, stage 5.0)
[2022-02-19 00:33:51,204] {subprocess.py:89} INFO - 22/02/19 00:33:51 INFO Executor: Finished task 2.0 in stage 5.0 (TID 25). 1493 bytes result sent to driver
[2022-02-19 00:33:51,206] {subprocess.py:89} INFO - 22/02/19 00:33:51 INFO TaskSetManager: Starting task 12.0 in stage 5.0 (TID 35) (2e85b2b3b082, executor driver, partition 12, ANY, 7499 bytes) taskResourceAssignments Map()
[2022-02-19 00:33:51,207] {subprocess.py:89} INFO - 22/02/19 00:33:51 INFO TaskSetManager: Finished task 2.0 in stage 5.0 (TID 25) in 1712 ms on 2e85b2b3b082 (executor driver) (5/19)
[2022-02-19 00:33:51,207] {subprocess.py:89} INFO - 22/02/19 00:33:51 INFO Executor: Running task 12.0 in stage 5.0 (TID 35)
[2022-02-19 00:33:51,497] {subprocess.py:89} INFO - 22/02/19 00:33:51 INFO DataWritingSparkTask: Commit authorized for partition 3 (task 26, attempt 0, stage 5.0)
[2022-02-19 00:33:51,511] {subprocess.py:89} INFO - 22/02/19 00:33:51 INFO DataWritingSparkTask: Commit authorized for partition 7 (task 30, attempt 0, stage 5.0)
[2022-02-19 00:33:51,514] {subprocess.py:89} INFO - 22/02/19 00:33:51 INFO DataWritingSparkTask: Committed partition 3 (task 26, attempt 0, stage 5.0)
[2022-02-19 00:33:51,515] {subprocess.py:89} INFO - 22/02/19 00:33:51 INFO Executor: Finished task 3.0 in stage 5.0 (TID 26). 1493 bytes result sent to driver
[2022-02-19 00:33:51,523] {subprocess.py:89} INFO - 22/02/19 00:33:51 INFO TaskSetManager: Starting task 13.0 in stage 5.0 (TID 36) (2e85b2b3b082, executor driver, partition 13, ANY, 7977 bytes) taskResourceAssignments Map()
[2022-02-19 00:33:51,523] {subprocess.py:89} INFO - 22/02/19 00:33:51 INFO TaskSetManager: Finished task 3.0 in stage 5.0 (TID 26) in 2027 ms on 2e85b2b3b082 (executor driver) (6/19)
[2022-02-19 00:33:51,526] {subprocess.py:89} INFO - 22/02/19 00:33:51 INFO DataWritingSparkTask: Committed partition 7 (task 30, attempt 0, stage 5.0)
[2022-02-19 00:33:51,526] {subprocess.py:89} INFO - 22/02/19 00:33:51 INFO Executor: Finished task 7.0 in stage 5.0 (TID 30). 1493 bytes result sent to driver
[2022-02-19 00:33:51,526] {subprocess.py:89} INFO - 22/02/19 00:33:51 INFO Executor: Running task 13.0 in stage 5.0 (TID 36)
[2022-02-19 00:33:51,574] {subprocess.py:89} INFO - 22/02/19 00:33:51 INFO TaskSetManager: Starting task 14.0 in stage 5.0 (TID 37) (2e85b2b3b082, executor driver, partition 14, ANY, 7976 bytes) taskResourceAssignments Map()
[2022-02-19 00:33:51,575] {subprocess.py:89} INFO - 22/02/19 00:33:51 INFO Executor: Running task 14.0 in stage 5.0 (TID 37)
[2022-02-19 00:33:51,578] {subprocess.py:89} INFO - 22/02/19 00:33:51 INFO TaskSetManager: Finished task 7.0 in stage 5.0 (TID 30) in 2079 ms on 2e85b2b3b082 (executor driver) (7/19)
[2022-02-19 00:33:51,617] {subprocess.py:89} INFO - 22/02/19 00:33:51 INFO DataWritingSparkTask: Commit authorized for partition 4 (task 27, attempt 0, stage 5.0)
[2022-02-19 00:33:51,626] {subprocess.py:89} INFO - 22/02/19 00:33:51 INFO DataWritingSparkTask: Committed partition 4 (task 27, attempt 0, stage 5.0)
[2022-02-19 00:33:51,628] {subprocess.py:89} INFO - 22/02/19 00:33:51 INFO Executor: Finished task 4.0 in stage 5.0 (TID 27). 1493 bytes result sent to driver
[2022-02-19 00:33:51,629] {subprocess.py:89} INFO - 22/02/19 00:33:51 INFO TaskSetManager: Starting task 15.0 in stage 5.0 (TID 38) (2e85b2b3b082, executor driver, partition 15, ANY, 7976 bytes) taskResourceAssignments Map()
[2022-02-19 00:33:51,631] {subprocess.py:89} INFO - 22/02/19 00:33:51 INFO Executor: Running task 15.0 in stage 5.0 (TID 38)
[2022-02-19 00:33:51,634] {subprocess.py:89} INFO - 22/02/19 00:33:51 INFO TaskSetManager: Finished task 4.0 in stage 5.0 (TID 27) in 2136 ms on 2e85b2b3b082 (executor driver) (8/19)
[2022-02-19 00:33:51,674] {subprocess.py:89} INFO - 22/02/19 00:33:51 INFO DataWritingSparkTask: Commit authorized for partition 10 (task 33, attempt 0, stage 5.0)
[2022-02-19 00:33:51,679] {subprocess.py:89} INFO - 22/02/19 00:33:51 INFO DataWritingSparkTask: Committed partition 10 (task 33, attempt 0, stage 5.0)
[2022-02-19 00:33:51,682] {subprocess.py:89} INFO - 22/02/19 00:33:51 INFO Executor: Finished task 10.0 in stage 5.0 (TID 33). 1450 bytes result sent to driver
[2022-02-19 00:33:51,682] {subprocess.py:89} INFO - 22/02/19 00:33:51 INFO TaskSetManager: Starting task 16.0 in stage 5.0 (TID 39) (2e85b2b3b082, executor driver, partition 16, ANY, 7501 bytes) taskResourceAssignments Map()
[2022-02-19 00:33:51,682] {subprocess.py:89} INFO - 22/02/19 00:33:51 INFO TaskSetManager: Finished task 10.0 in stage 5.0 (TID 33) in 860 ms on 2e85b2b3b082 (executor driver) (9/19)
[2022-02-19 00:33:51,683] {subprocess.py:89} INFO - 22/02/19 00:33:51 INFO Executor: Running task 16.0 in stage 5.0 (TID 39)
[2022-02-19 00:33:52,228] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO DataWritingSparkTask: Commit authorized for partition 9 (task 32, attempt 0, stage 5.0)
[2022-02-19 00:33:52,240] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO DataWritingSparkTask: Committed partition 9 (task 32, attempt 0, stage 5.0)
[2022-02-19 00:33:52,241] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO Executor: Finished task 9.0 in stage 5.0 (TID 32). 1493 bytes result sent to driver
[2022-02-19 00:33:52,245] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO TaskSetManager: Starting task 17.0 in stage 5.0 (TID 40) (2e85b2b3b082, executor driver, partition 17, ANY, 7260 bytes) taskResourceAssignments Map()
[2022-02-19 00:33:52,245] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO TaskSetManager: Finished task 9.0 in stage 5.0 (TID 32) in 1446 ms on 2e85b2b3b082 (executor driver) (10/19)
[2022-02-19 00:33:52,245] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO Executor: Running task 17.0 in stage 5.0 (TID 40)
[2022-02-19 00:33:52,289] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO DataWritingSparkTask: Commit authorized for partition 11 (task 34, attempt 0, stage 5.0)
[2022-02-19 00:33:52,300] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO DataWritingSparkTask: Committed partition 11 (task 34, attempt 0, stage 5.0)
[2022-02-19 00:33:52,302] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO Executor: Finished task 11.0 in stage 5.0 (TID 34). 1493 bytes result sent to driver
[2022-02-19 00:33:52,304] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO TaskSetManager: Starting task 18.0 in stage 5.0 (TID 41) (2e85b2b3b082, executor driver, partition 18, ANY, 8807 bytes) taskResourceAssignments Map()
[2022-02-19 00:33:52,307] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO Executor: Running task 18.0 in stage 5.0 (TID 41)
[2022-02-19 00:33:52,307] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO TaskSetManager: Finished task 11.0 in stage 5.0 (TID 34) in 1229 ms on 2e85b2b3b082 (executor driver) (11/19)
[2022-02-19 00:33:52,330] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO DataWritingSparkTask: Commit authorized for partition 13 (task 36, attempt 0, stage 5.0)
[2022-02-19 00:33:52,343] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO DataWritingSparkTask: Committed partition 13 (task 36, attempt 0, stage 5.0)
[2022-02-19 00:33:52,344] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO Executor: Finished task 13.0 in stage 5.0 (TID 36). 1493 bytes result sent to driver
[2022-02-19 00:33:52,347] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO TaskSetManager: Finished task 13.0 in stage 5.0 (TID 36) in 823 ms on 2e85b2b3b082 (executor driver) (12/19)
[2022-02-19 00:33:52,352] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO DataWritingSparkTask: Commit authorized for partition 14 (task 37, attempt 0, stage 5.0)
[2022-02-19 00:33:52,357] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO DataWritingSparkTask: Committed partition 14 (task 37, attempt 0, stage 5.0)
[2022-02-19 00:33:52,358] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO Executor: Finished task 14.0 in stage 5.0 (TID 37). 1493 bytes result sent to driver
[2022-02-19 00:33:52,370] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO TaskSetManager: Finished task 14.0 in stage 5.0 (TID 37) in 797 ms on 2e85b2b3b082 (executor driver) (13/19)
[2022-02-19 00:33:52,502] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO DataWritingSparkTask: Commit authorized for partition 12 (task 35, attempt 0, stage 5.0)
[2022-02-19 00:33:52,507] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO DataWritingSparkTask: Commit authorized for partition 15 (task 38, attempt 0, stage 5.0)
[2022-02-19 00:33:52,511] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO DataWritingSparkTask: Committed partition 12 (task 35, attempt 0, stage 5.0)
[2022-02-19 00:33:52,513] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO Executor: Finished task 12.0 in stage 5.0 (TID 35). 1493 bytes result sent to driver
[2022-02-19 00:33:52,515] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO TaskSetManager: Finished task 12.0 in stage 5.0 (TID 35) in 1309 ms on 2e85b2b3b082 (executor driver) (14/19)
[2022-02-19 00:33:52,525] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO DataWritingSparkTask: Committed partition 15 (task 38, attempt 0, stage 5.0)
[2022-02-19 00:33:52,525] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO Executor: Finished task 15.0 in stage 5.0 (TID 38). 1493 bytes result sent to driver
[2022-02-19 00:33:52,526] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO TaskSetManager: Finished task 15.0 in stage 5.0 (TID 38) in 898 ms on 2e85b2b3b082 (executor driver) (15/19)
[2022-02-19 00:33:52,584] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO DataWritingSparkTask: Commit authorized for partition 16 (task 39, attempt 0, stage 5.0)
[2022-02-19 00:33:52,599] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO DataWritingSparkTask: Committed partition 16 (task 39, attempt 0, stage 5.0)
[2022-02-19 00:33:52,600] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO Executor: Finished task 16.0 in stage 5.0 (TID 39). 1493 bytes result sent to driver
[2022-02-19 00:33:52,606] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO TaskSetManager: Finished task 16.0 in stage 5.0 (TID 39) in 926 ms on 2e85b2b3b082 (executor driver) (16/19)
[2022-02-19 00:33:52,725] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO DataWritingSparkTask: Commit authorized for partition 8 (task 31, attempt 0, stage 5.0)
[2022-02-19 00:33:52,729] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO DataWritingSparkTask: Commit authorized for partition 18 (task 41, attempt 0, stage 5.0)
[2022-02-19 00:33:52,733] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO DataWritingSparkTask: Committed partition 18 (task 41, attempt 0, stage 5.0)
[2022-02-19 00:33:52,734] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO Executor: Finished task 18.0 in stage 5.0 (TID 41). 1450 bytes result sent to driver
[2022-02-19 00:33:52,737] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO DataWritingSparkTask: Committed partition 8 (task 31, attempt 0, stage 5.0)
[2022-02-19 00:33:52,737] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO TaskSetManager: Finished task 18.0 in stage 5.0 (TID 41) in 433 ms on 2e85b2b3b082 (executor driver) (17/19)
[2022-02-19 00:33:52,738] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO Executor: Finished task 8.0 in stage 5.0 (TID 31). 1493 bytes result sent to driver
[2022-02-19 00:33:52,739] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO TaskSetManager: Finished task 8.0 in stage 5.0 (TID 31) in 2052 ms on 2e85b2b3b082 (executor driver) (18/19)
[2022-02-19 00:33:52,764] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO DataWritingSparkTask: Commit authorized for partition 17 (task 40, attempt 0, stage 5.0)
[2022-02-19 00:33:52,766] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO DataWritingSparkTask: Committed partition 17 (task 40, attempt 0, stage 5.0)
[2022-02-19 00:33:52,767] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO Executor: Finished task 17.0 in stage 5.0 (TID 40). 1450 bytes result sent to driver
[2022-02-19 00:33:52,767] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO TaskSetManager: Finished task 17.0 in stage 5.0 (TID 40) in 525 ms on 2e85b2b3b082 (executor driver) (19/19)
[2022-02-19 00:33:52,767] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2022-02-19 00:33:52,768] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO DAGScheduler: ResultStage 5 (save at NativeMethodAccessorImpl.java:0) finished in 3.289 s
[2022-02-19 00:33:52,768] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2022-02-19 00:33:52,769] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2022-02-19 00:33:52,769] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO DAGScheduler: Job 5 finished: save at NativeMethodAccessorImpl.java:0, took 3.294164 s
[2022-02-19 00:33:52,769] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO AppendDataExec: Data source write support CassandraBulkWrite(org.apache.spark.sql.SparkSession@693802a4,com.datastax.spark.connector.cql.CassandraConnector@332fa6c4,TableDef(test,days_worked_by_previous_employees_by_job_title,ArrayBuffer(ColumnDef(job_title,PartitionKeyColumn,VarCharType)),ArrayBuffer(ColumnDef(employee_id,ClusteringColumn(0,ASC),UUIDType)),Stream(ColumnDef(days_worked,RegularColumn,IntType), ColumnDef(employee_name,RegularColumn,VarCharType)),Stream(),false,false,Map()),WriteConf(BytesInBatch(1024),1000,Partition,LOCAL_QUORUM,false,false,5,None,TTLOption(DefaultValue),TimestampOption(DefaultValue),true,None),StructType(StructField(job_title,StringType,false), StructField(employee_id,StringType,true), StructField(employee_name,StringType,true), StructField(days_worked,IntegerType,true)),org.apache.spark.SparkConf@6de20544) is committing.
[2022-02-19 00:33:52,770] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO AppendDataExec: Data source write support CassandraBulkWrite(org.apache.spark.sql.SparkSession@693802a4,com.datastax.spark.connector.cql.CassandraConnector@332fa6c4,TableDef(test,days_worked_by_previous_employees_by_job_title,ArrayBuffer(ColumnDef(job_title,PartitionKeyColumn,VarCharType)),ArrayBuffer(ColumnDef(employee_id,ClusteringColumn(0,ASC),UUIDType)),Stream(ColumnDef(days_worked,RegularColumn,IntType), ColumnDef(employee_name,RegularColumn,VarCharType)),Stream(),false,false,Map()),WriteConf(BytesInBatch(1024),1000,Partition,LOCAL_QUORUM,false,false,5,None,TTLOption(DefaultValue),TimestampOption(DefaultValue),true,None),StructType(StructField(job_title,StringType,false), StructField(employee_id,StringType,true), StructField(employee_name,StringType,true), StructField(days_worked,IntegerType,true)),org.apache.spark.SparkConf@6de20544) committed.
[2022-02-19 00:33:52,782] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO SparkUI: Stopped Spark web UI at http://2e85b2b3b082:4040
[2022-02-19 00:33:52,807] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2022-02-19 00:33:52,823] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO MemoryStore: MemoryStore cleared
[2022-02-19 00:33:52,824] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO BlockManager: BlockManager stopped
[2022-02-19 00:33:52,826] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO BlockManagerMaster: BlockManagerMaster stopped
[2022-02-19 00:33:52,828] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2022-02-19 00:33:52,836] {subprocess.py:89} INFO - 22/02/19 00:33:52 INFO SparkContext: Successfully stopped SparkContext
[2022-02-19 00:33:53,087] {subprocess.py:89} INFO - 22/02/19 00:33:53 INFO ShutdownHookManager: Shutdown hook called
[2022-02-19 00:33:53,089] {subprocess.py:89} INFO - 22/02/19 00:33:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc/pyspark-c0ec71dc-e017-4861-a014-d11383a94052
[2022-02-19 00:33:53,094] {subprocess.py:89} INFO - 22/02/19 00:33:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-44137500-ab05-4992-94bf-b0350deb6b51
[2022-02-19 00:33:53,101] {subprocess.py:89} INFO - 22/02/19 00:33:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-07aff19d-d69b-432c-9311-5fcaa36a30bc
[2022-02-19 00:33:53,151] {subprocess.py:89} INFO - 22/02/19 00:33:53 INFO CassandraConnector: Disconnected from Cassandra cluster.
[2022-02-19 00:33:53,152] {subprocess.py:89} INFO - 22/02/19 00:33:53 INFO SerialShutdownHooks: Successfully executed shutdown hook: Clearing session cache for C* connector
[2022-02-19 00:33:53,228] {subprocess.py:93} INFO - Command exited with return code 0
[2022-02-19 00:33:53,252] {taskinstance.py:1277} INFO - Marking task as SUCCESS. dag_id=spark-test-cassandra, task_id=load_and_write_job, execution_date=20220219T003201, start_date=20220219T003237, end_date=20220219T003353
[2022-02-19 00:33:53,311] {local_task_job.py:154} INFO - Task exited with return code 0
[2022-02-19 00:33:53,340] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check
